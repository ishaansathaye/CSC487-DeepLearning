{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4X00o6AYKEl"
      },
      "source": [
        "### Lab 7.1: Bag of Words Model\n",
        "\n",
        "In this lab you will use the bag of words model to learn author attribution with a [dataset of texts from Victorian authors](https://github.com/agungor2/Authorship_Attribution?tab=readme-ov-file)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "eNzZsS19pFxI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import sklearn\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6EgVxxbcGBN"
      },
      "source": [
        "Here we download the CSV file containing the text snippets and author IDs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "nHGLy49dpcvv"
      },
      "outputs": [],
      "source": [
        "!wget --no-clobber -O Gungor_2018_VictorianAuthorAttribution_data-train.csv -q \"https://www.dropbox.com/scl/fi/emk9db05t9u8yzgrjje7t/Gungor_2018_VictorianAuthorAttribution_data-train.csv?rlkey=kzvbl0mbpnrpjr4c3q18le6w2&dl=1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "NEczcgxSplh6",
        "outputId": "8ddd53ae-54ba-4628-ff7f-d0bf25c5c895"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>author</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ou have time to listen i will give you the ent...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>wish for solitude he was twenty years of age a...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>and the skirt blew in perfect freedom about th...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>of san and the rows of shops opposite impresse...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>an hour s walk was as tiresome as three in a s...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  author\n",
              "0  ou have time to listen i will give you the ent...       1\n",
              "1  wish for solitude he was twenty years of age a...       1\n",
              "2  and the skirt blew in perfect freedom about th...       1\n",
              "3  of san and the rows of shops opposite impresse...       1\n",
              "4  an hour s walk was as tiresome as three in a s...       1"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv('Gungor_2018_VictorianAuthorAttribution_data-train.csv', encoding = \"ISO-8859-1\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "PGpnyq9EpvE4"
      },
      "outputs": [],
      "source": [
        "text = list(df['text'])\n",
        "labels = df['author'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "id": "BPDH_z93QOxn",
        "outputId": "a259aa9c-3e07-4f5a-be77-8789961d07eb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'ou have time to listen i will give you the entire story he said it may form the basis of a future novel and prove quite as interesting as one of your own invention i had the time to listen of course one has time for anything and everything agreeable in the best place to hear the tale was in a victoria and with my good on the box with the coachman we set out at once on a drive to the as the recital was only half through when we reached the house we postponed the remainder while we stopped there for an excellent lunch on the way back to my friend continued and finished the story it was indeed quite suitable for use and i told my friend with thanks that i should at once put it in shape for my readers i said i should make a few alterations in it for the sake of dramatic interest but in the main would follow the lines he had given me it would spoil my romance were i to answer on this page the question that must be uppermost in the reader s mind i have already revealed almost too much of the plot for the rest i must refer you t without more to the chapters that follow chapter i in a it was very early in the morning and the responded to the call of the young american on the the rowed leisurely to the bank for the gentry to which he belongs does not easily get excited and helped his fare into the with a grace inherited from generations of polite ancestors where he asked in his italian and the young man who hardly knew a word of the language had no difficulty in the meaning of the question anywhere he answered with a wave of his hand as easily understood as the term used by the other he wanted an early row among the of and as he had been in the silent city love gone astray but a short time one direction was as agreeable to him as another the took his long oar and began to his craft by those strange sweeping motions that so interest and puzzle one unused to this style of standing well back toward the stern he sent the beautiful creature of which he seemed a part as gracefully through the water as any swan he rowed slowly both from preference and because it was evident that haste was not desired by his passenger he rowed because there is no other mode known to the of from the attendant of a to the freight who brings a load of from the or of vegetables from the islands where the market gardens are for a while the course of the boat lay along the grand canal it passed under the venerable as solid as london bridge in effect one massive stone that will be as far as human judgment can foresee until the earth is in its final on either side of the canal long lines of palaces shone in the early light their occupants for the most part yet invisible was still asleep lovely as she is at all times this stately creature is never so pretty as when in repose a glide along her watery streets just before sunrise is like moving silently through a garden where lie in slumber in there is no wheeled carriage of any description not a horse mule ox goat sheep puts his foot upon her the station at which you arrive by train is at an extreme corner of the in a city and even its necessary noise is tempered by the surroundings the only of passenger or goods service are the boats which make hardly more disturbance for the ear than a fish passing over the same route every sound and when the city she is capable of many sounds proceeds from the voices of individuals or the of the sacred that are fed by thousands at all hours in the of st mark the of various wares seems to feel that it is incumbent upon him to mock the echoes of the winding over which one may stroll the themselves when there is a possible occasion cry out to each other in weird tones especially at narrow of the side to prevent collision with craft approaching silently from beyond the stone and brick of a corner besides in protest of the natural stillness of their city they quarrel for hours in front of the principal hotels with as much effect as a parcel of highly birds in an african forest but for these things would be as quiet as the in which the pin is about to drop or as a in a superstitious neighborhood at the morning hour when young gray rode in his under the these noises had only faintly begun and the delight he felt in his excursion was great he wanted the effect of solitude with the hidden from sight by his position the boat seemed by a sail or the force of a tide until men have mastered the currents of the love gone upper air and can voyage whither they please in the there will be no effect so nearly like it as to float on the bosom of a canal the drift the current of a river does not give the same impression for there is a tiresome row in prospect before the starting point can be regained no boat by machinery even the tidy little equals it for the noise of the wheels cannot entirely be and the smell of the themselves in spite of all precaution to the nostrils the may give more excitement as she bird like across the waters of the sea but she does not lull the senses and transport the into another world from which he may return at pleasure only the does this why did gray '"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMm2D-6CcMpj"
      },
      "source": [
        "### Exercises\n",
        "\n",
        "1. Use the `CountVectorizer` from `sklearn.feature_extraction.text` to produce a term frequency vector for each text.  Set `max_features=1000` to only use the top 1000 terms.\n",
        "\n",
        "Prepare a 90/10 train-test split `random_state=42`.\n",
        "\n",
        "Train the default `MLPCLassifier` from `sklearn.neural_network` on the data and report the train and test accuracy.  You can use the argument `verbose=True` to `MLPClassifier` to monitor training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "bPKXRSRctLGr"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 1.92049611\n",
            "Iteration 2, loss = 0.50725217\n",
            "Iteration 3, loss = 0.28534340\n",
            "Iteration 4, loss = 0.19642350\n",
            "Iteration 5, loss = 0.14355440\n",
            "Iteration 6, loss = 0.10809862\n",
            "Iteration 7, loss = 0.08454068\n",
            "Iteration 8, loss = 0.06441515\n",
            "Iteration 9, loss = 0.04846363\n",
            "Iteration 10, loss = 0.03714305\n",
            "Iteration 11, loss = 0.02882209\n",
            "Iteration 12, loss = 0.02181749\n",
            "Iteration 13, loss = 0.01952588\n",
            "Iteration 14, loss = 0.01487900\n",
            "Iteration 15, loss = 0.01151154\n",
            "Iteration 16, loss = 0.01048287\n",
            "Iteration 17, loss = 0.00885181\n",
            "Iteration 18, loss = 0.00858167\n",
            "Iteration 19, loss = 0.00888093\n",
            "Iteration 20, loss = 0.00837009\n",
            "Iteration 21, loss = 0.00816079\n",
            "Iteration 22, loss = 0.01030391\n",
            "Iteration 23, loss = 0.00885173\n",
            "Iteration 24, loss = 0.00919736\n",
            "Iteration 25, loss = 0.01052904\n",
            "Iteration 26, loss = 0.00779272\n",
            "Iteration 27, loss = 0.00746545\n",
            "Iteration 28, loss = 0.00432913\n",
            "Iteration 29, loss = 0.00455537\n",
            "Iteration 30, loss = 0.00497592\n",
            "Iteration 31, loss = 0.01959066\n",
            "Iteration 32, loss = 0.02440855\n",
            "Iteration 33, loss = 0.00792086\n",
            "Iteration 34, loss = 0.00282696\n",
            "Iteration 35, loss = 0.00232154\n",
            "Iteration 36, loss = 0.00187576\n",
            "Iteration 37, loss = 0.00189883\n",
            "Iteration 38, loss = 0.00206844\n",
            "Iteration 39, loss = 0.00334212\n",
            "Iteration 40, loss = 0.03471793\n",
            "Iteration 41, loss = 0.00858244\n",
            "Iteration 42, loss = 0.00522422\n",
            "Iteration 43, loss = 0.00275421\n",
            "Iteration 44, loss = 0.00224740\n",
            "Iteration 45, loss = 0.00198194\n",
            "Iteration 46, loss = 0.00195527\n",
            "Iteration 47, loss = 0.00330567\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Train accuracy: 0.9997102049265163\n",
            "Test accuracy: 0.9396423248882265\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "vectorizer = CountVectorizer(max_features=1000)\n",
        "X = vectorizer.fit_transform(text).toarray()\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.1, random_state=42)\n",
        "clf = MLPClassifier(verbose=True)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_train_pred = clf.predict(X_train)\n",
        "y_test_pred = clf.predict(X_test)\n",
        "print(\"Train accuracy:\", accuracy_score(y_train, y_train_pred))\n",
        "print(\"Test accuracy:\", accuracy_score(y_test, y_test_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bscBHmXududo"
      },
      "source": [
        "\n",
        "2. Repeat the steps but using `TfidfVectorizer` to produce term frequency - inverse document frequency vectors.\n",
        "\n",
        "Does the IDF weighting improve the results?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "oVHb29MztMlN"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 3.01926992\n",
            "Iteration 2, loss = 1.99989702\n",
            "Iteration 3, loss = 1.38074143\n",
            "Iteration 4, loss = 0.99334759\n",
            "Iteration 5, loss = 0.75812567\n",
            "Iteration 6, loss = 0.60819539\n",
            "Iteration 7, loss = 0.50657658\n",
            "Iteration 8, loss = 0.43203296\n",
            "Iteration 9, loss = 0.37583316\n",
            "Iteration 10, loss = 0.33166791\n",
            "Iteration 11, loss = 0.29580513\n",
            "Iteration 12, loss = 0.26717061\n",
            "Iteration 13, loss = 0.24178459\n",
            "Iteration 14, loss = 0.22146274\n",
            "Iteration 15, loss = 0.20319301\n",
            "Iteration 16, loss = 0.18773918\n",
            "Iteration 17, loss = 0.17354409\n",
            "Iteration 18, loss = 0.16096451\n",
            "Iteration 19, loss = 0.15036259\n",
            "Iteration 20, loss = 0.14057153\n",
            "Iteration 21, loss = 0.13134682\n",
            "Iteration 22, loss = 0.12332095\n",
            "Iteration 23, loss = 0.11578058\n",
            "Iteration 24, loss = 0.10905762\n",
            "Iteration 25, loss = 0.10293909\n",
            "Iteration 26, loss = 0.09675139\n",
            "Iteration 27, loss = 0.09191076\n",
            "Iteration 28, loss = 0.08682815\n",
            "Iteration 29, loss = 0.08222857\n",
            "Iteration 30, loss = 0.07808349\n",
            "Iteration 31, loss = 0.07403504\n",
            "Iteration 32, loss = 0.07037870\n",
            "Iteration 33, loss = 0.06678961\n",
            "Iteration 34, loss = 0.06360235\n",
            "Iteration 35, loss = 0.06059662\n",
            "Iteration 36, loss = 0.05779211\n",
            "Iteration 37, loss = 0.05526481\n",
            "Iteration 38, loss = 0.05278038\n",
            "Iteration 39, loss = 0.05039010\n",
            "Iteration 40, loss = 0.04822532\n",
            "Iteration 41, loss = 0.04622769\n",
            "Iteration 42, loss = 0.04419354\n",
            "Iteration 43, loss = 0.04227228\n",
            "Iteration 44, loss = 0.04087885\n",
            "Iteration 45, loss = 0.03917516\n",
            "Iteration 46, loss = 0.03764479\n",
            "Iteration 47, loss = 0.03644593\n",
            "Iteration 48, loss = 0.03485121\n",
            "Iteration 49, loss = 0.03376049\n",
            "Iteration 50, loss = 0.03242059\n",
            "Iteration 51, loss = 0.03150136\n",
            "Iteration 52, loss = 0.03029939\n",
            "Iteration 53, loss = 0.02920145\n",
            "Iteration 54, loss = 0.02850449\n",
            "Iteration 55, loss = 0.02757160\n",
            "Iteration 56, loss = 0.02689400\n",
            "Iteration 57, loss = 0.02587398\n",
            "Iteration 58, loss = 0.02517555\n",
            "Iteration 59, loss = 0.02450810\n",
            "Iteration 60, loss = 0.02385456\n",
            "Iteration 61, loss = 0.02317048\n",
            "Iteration 62, loss = 0.02276326\n",
            "Iteration 63, loss = 0.02225334\n",
            "Iteration 64, loss = 0.02164240\n",
            "Iteration 65, loss = 0.02134963\n",
            "Iteration 66, loss = 0.02075246\n",
            "Iteration 67, loss = 0.02042061\n",
            "Iteration 68, loss = 0.02002851\n",
            "Iteration 69, loss = 0.01966988\n",
            "Iteration 70, loss = 0.01937459\n",
            "Iteration 71, loss = 0.01911439\n",
            "Iteration 72, loss = 0.01856798\n",
            "Iteration 73, loss = 0.01838537\n",
            "Iteration 74, loss = 0.01800211\n",
            "Iteration 75, loss = 0.01771743\n",
            "Iteration 76, loss = 0.01756790\n",
            "Iteration 77, loss = 0.01733920\n",
            "Iteration 78, loss = 0.01709156\n",
            "Iteration 79, loss = 0.01681938\n",
            "Iteration 80, loss = 0.01679176\n",
            "Iteration 81, loss = 0.01654683\n",
            "Iteration 82, loss = 0.01640254\n",
            "Iteration 83, loss = 0.01610576\n",
            "Iteration 84, loss = 0.01608877\n",
            "Iteration 85, loss = 0.01578394\n",
            "Iteration 86, loss = 0.01570096\n",
            "Iteration 87, loss = 0.01547324\n",
            "Iteration 88, loss = 0.01537071\n",
            "Iteration 89, loss = 0.01514489\n",
            "Iteration 90, loss = 0.01512967\n",
            "Iteration 91, loss = 0.01508220\n",
            "Iteration 92, loss = 0.01498815\n",
            "Iteration 93, loss = 0.01471959\n",
            "Iteration 94, loss = 0.01463195\n",
            "Iteration 95, loss = 0.01453648\n",
            "Iteration 96, loss = 0.01451142\n",
            "Iteration 97, loss = 0.01427710\n",
            "Iteration 98, loss = 0.01423178\n",
            "Iteration 99, loss = 0.01412490\n",
            "Iteration 100, loss = 0.01406347\n",
            "Iteration 101, loss = 0.01399728\n",
            "Iteration 102, loss = 0.01399077\n",
            "Iteration 103, loss = 0.01373874\n",
            "Iteration 104, loss = 0.01375689\n",
            "Iteration 105, loss = 0.01361383\n",
            "Iteration 106, loss = 0.01355425\n",
            "Iteration 107, loss = 0.01349622\n",
            "Iteration 108, loss = 0.01344451\n",
            "Iteration 109, loss = 0.01327272\n",
            "Iteration 110, loss = 0.01327616\n",
            "Iteration 111, loss = 0.01316454\n",
            "Iteration 112, loss = 0.01310531\n",
            "Iteration 113, loss = 0.01322833\n",
            "Iteration 114, loss = 0.01315052\n",
            "Iteration 115, loss = 0.01284950\n",
            "Iteration 116, loss = 0.01301058\n",
            "Iteration 117, loss = 0.01273437\n",
            "Iteration 118, loss = 0.01294990\n",
            "Iteration 119, loss = 0.01259072\n",
            "Iteration 120, loss = 0.01254905\n",
            "Iteration 121, loss = 0.01249506\n",
            "Iteration 122, loss = 0.01244038\n",
            "Iteration 123, loss = 0.01249882\n",
            "Iteration 124, loss = 0.01238018\n",
            "Iteration 125, loss = 0.01240663\n",
            "Iteration 126, loss = 0.01244465\n",
            "Iteration 127, loss = 0.01222644\n",
            "Iteration 128, loss = 0.01211092\n",
            "Iteration 129, loss = 0.01197684\n",
            "Iteration 130, loss = 0.01207247\n",
            "Iteration 131, loss = 0.01198085\n",
            "Iteration 132, loss = 0.01194278\n",
            "Iteration 133, loss = 0.01198051\n",
            "Iteration 134, loss = 0.01204907\n",
            "Iteration 135, loss = 0.01184325\n",
            "Iteration 136, loss = 0.01188833\n",
            "Iteration 137, loss = 0.01178470\n",
            "Iteration 138, loss = 0.01175555\n",
            "Iteration 139, loss = 0.01169322\n",
            "Iteration 140, loss = 0.01173824\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Train accuracy: 0.9999172014075761\n",
            "Test accuracy: 0.9359165424739195\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features=1000)\n",
        "X = vectorizer.fit_transform(text).toarray()\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.1, random_state=42)\n",
        "clf = MLPClassifier(verbose=True)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_train_pred = clf.predict(X_train)\n",
        "y_test_pred = clf.predict(X_test)\n",
        "print(\"Train accuracy:\", accuracy_score(y_train, y_train_pred))\n",
        "print(\"Test accuracy:\", accuracy_score(y_test, y_test_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "IDF weighting did not have a significant impact on the results since both models achieved similar accuracy scores."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
