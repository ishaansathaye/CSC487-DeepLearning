{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 8.3 Text generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab you will finish building your RNN text generator.  I found that this code actually runs pretty quickly on my MacBook without GPU acceleration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "seq_len = 20\n",
    "hidden_size = 100\n",
    "batch_size = 32\n",
    "lr = 3e-4\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchmetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the code to download and prepare the sonnet dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘sonnets.txt’ already there; not retrieving.\n"
     ]
    }
   ],
   "source": [
    "!wget --no-clobber \"https://www.dropbox.com/scl/fi/7r68l64ijemidyb9lf80q/sonnets.txt?rlkey=udb47coatr2zbrk31hsfbr22y&dl=1\" -O sonnets.txt\n",
    "text = (open(\"sonnets.txt\").read())\n",
    "text = text.lower().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i\n",
      "\n",
      " from fairest creatures we desire increase,\n",
      " that thereby beauty's rose might never die,\n",
      " but as the riper should by time decease,\n",
      " his tender heir might bear his memory:\n",
      " but thou, contracted to thine own bright eyes,\n",
      " feed'st thy light's flame with self-substantial fuel,\n",
      " making a famine where abundance lies,\n",
      " thy self thy foe, to thy sweet self too cruel:\n",
      " thou that art now the world's fresh ornament,\n",
      " and only herald to the gaudy spring,\n",
      " within thine own bud buriest thy content,\n",
      " and tender churl mak'st waste in niggarding:\n",
      "   pity the world, or else this glutton be,\n",
      "   to eat the world's due, by the grave and thee.\n",
      "\n",
      " ii\n",
      "\n",
      " when forty winters shall besiege thy brow,\n",
      " and dig deep trenches in thy beauty's field,\n",
      " thy youth's proud livery so gazed on now,\n",
      " will be a tatter'd weed of small worth held:\n",
      " then being asked, where all thy beauty lies,\n",
      " where all the treasure of thy lusty days;\n",
      " to say, within thine own deep sunken eyes,\n",
      " were an all-eating shame, and thriftless praise.\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's my solution for the `CharacterDataset` class.\n",
    "\n",
    "Note that it returns an entire sequence of tokens for the target (unlike what we did on Monday where it only output a single token for the target.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharacterDataset(Dataset):\n",
    "  def __init__(self,text,seq_len=100,device='cpu'):\n",
    "    \"\"\"\n",
    "    Initialize a dataset using character tokenization.\n",
    "    Arguments:\n",
    "      text: a string containing the dataset\n",
    "      seq_len: sequence length provided by __getitem__\n",
    "      device: device for PyTorch tensors\n",
    "    \"\"\"\n",
    "    self.text = text\n",
    "    self.seq_len = seq_len\n",
    "    self.vocabulary = ''.join(sorted(list(set(text))))\n",
    "    self.index_to_char = {n:char for n, char in enumerate(self.vocabulary)}\n",
    "    self.char_to_index = {char:n for n, char in enumerate(self.vocabulary)}\n",
    "    self.device = device\n",
    "\n",
    "  def __len__(self):\n",
    "    \"\"\" Return the length of sequences in the dataset. \"\"\"\n",
    "    return len(self.text)-self.seq_len-1\n",
    "\n",
    "  def __getitem__(self,idx):\n",
    "    \"\"\" Return the input and target sequences starting at given index. \"\"\"\n",
    "\n",
    "    text = self.text[idx:idx+self.seq_len+1]\n",
    "    tokens = self.encode(text)\n",
    "\n",
    "    return torch.tensor(tokens[:-1],device=self.device),torch.tensor(tokens[1:],device=self.device)\n",
    "  \n",
    "  def encode(self,text):\n",
    "    \"\"\" Encode a string to a list of integer tokens. \"\"\"\n",
    "    return list(map(self.char_to_index.get,text))\n",
    "\n",
    "  def decode(self,tokens):\n",
    "    \"\"\" Decode a list of token integers into a string. \"\"\"\n",
    "    return ''.join(list(map(self.index_to_char.get,tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = CharacterDataset(text,seq_len=seq_len,device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[38,\n",
       " 20,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 17,\n",
       " 29,\n",
       " 26,\n",
       " 24,\n",
       " 1,\n",
       " 17,\n",
       " 12,\n",
       " 20,\n",
       " 29,\n",
       " 16,\n",
       " 30,\n",
       " 31,\n",
       " 1,\n",
       " 14,\n",
       " 29,\n",
       " 16,\n",
       " 12,\n",
       " 31,\n",
       " 32,\n",
       " 29,\n",
       " 16,\n",
       " 30,\n",
       " 1,\n",
       " 34,\n",
       " 16,\n",
       " 1,\n",
       " 15,\n",
       " 16,\n",
       " 30,\n",
       " 20,\n",
       " 29,\n",
       " 16,\n",
       " 1,\n",
       " 20,\n",
       " 25,\n",
       " 14,\n",
       " 29,\n",
       " 16,\n",
       " 12,\n",
       " 30,\n",
       " 16,\n",
       " 6,\n",
       " 0,\n",
       " 1,\n",
       " 31,\n",
       " 19,\n",
       " 12,\n",
       " 31,\n",
       " 1,\n",
       " 31,\n",
       " 19,\n",
       " 16,\n",
       " 29,\n",
       " 16,\n",
       " 13,\n",
       " 36,\n",
       " 1,\n",
       " 13,\n",
       " 16,\n",
       " 12,\n",
       " 32,\n",
       " 31,\n",
       " 36,\n",
       " 3,\n",
       " 30,\n",
       " 1,\n",
       " 29,\n",
       " 26,\n",
       " 30,\n",
       " 16,\n",
       " 1,\n",
       " 24,\n",
       " 20,\n",
       " 18,\n",
       " 19,\n",
       " 31,\n",
       " 1,\n",
       " 25,\n",
       " 16,\n",
       " 33,\n",
       " 16,\n",
       " 29,\n",
       " 1,\n",
       " 15,\n",
       " 20,\n",
       " 16,\n",
       " 6,\n",
       " 0,\n",
       " 1,\n",
       " 13,\n",
       " 32,\n",
       " 31,\n",
       " 1,\n",
       " 12,\n",
       " 30]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.encode(text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i\n",
      "\n",
      " from fairest creatures we desire increase,\n",
      " that thereby beauty's rose might never die,\n",
      " but as\n"
     ]
    }
   ],
   "source": [
    "print(ds.decode(ds.encode(text[:100])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([20]), torch.Size([20]))"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = ds[0]\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(ds,shuffle=True,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's my solution for the recurrent neural network (RNN) implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharacterRNN(nn.Module):\n",
    "  def __init__(self,vocabulary_size,hidden_size):\n",
    "    super().__init__()\n",
    "    self.embedding = nn.Embedding(vocabulary_size,hidden_size)\n",
    "    self.hidden_size = hidden_size\n",
    "    self.U = nn.Linear(hidden_size,hidden_size)\n",
    "    self.W = nn.Linear(hidden_size,hidden_size)\n",
    "    self.act = nn.SiLU()\n",
    "    self.V = nn.Linear(hidden_size,vocabulary_size)\n",
    "\n",
    "  def forward(self,x):\n",
    "    x = self.embedding(x)\n",
    "    B,N = x.shape[:2]\n",
    "    h = torch.zeros(B,self.hidden_size).to(x.device)\n",
    "    Ux = self.U(x)\n",
    "    y = []\n",
    "    for i in range(N):\n",
    "      Wh = self.W(h)\n",
    "      h = self.act(Ux[:,i] + Wh)\n",
    "      y.append(self.V(h))\n",
    "    return torch.stack(y,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CharacterRNN(len(ds.vocabulary),hidden_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 20]), torch.Size([32, 20]))"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_batch, y_batch = next(iter(dl))\n",
    "x_batch.shape, y_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 20, 39])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x_batch).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally here is my code to train the model.\n",
    "\n",
    "Note that I needed to use `.view()` to reshape the model output and target, becuase the loss and metric functions want the data to have shape [B,C] not [B,N,C]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MulticlassAccuracy()"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt = torch.optim.Adam(model.parameters(),lr=lr)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "metric = torchmetrics.classification.Accuracy(task=\"multiclass\", num_classes=len(ds.vocabulary))\n",
    "metric.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 1196/3060 [57:55<1:30:16,  2.91s/it]\n",
      "100%|██████████| 3060/3060 [00:12<00:00, 240.84it/s]\n",
      "100%|██████████| 3060/3060 [00:04<00:00, 627.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: 0.4673592150211334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3060/3060 [00:12<00:00, 249.40it/s]\n",
      "100%|██████████| 3060/3060 [00:04<00:00, 680.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: 0.49048253893852234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3060/3060 [00:11<00:00, 255.97it/s]\n",
      "100%|██████████| 3060/3060 [00:04<00:00, 688.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2: 0.5028749108314514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3060/3060 [00:12<00:00, 240.21it/s]\n",
      "100%|██████████| 3060/3060 [00:04<00:00, 671.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3: 0.5104367733001709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3060/3060 [00:11<00:00, 263.00it/s]\n",
      "100%|██████████| 3060/3060 [00:04<00:00, 699.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4: 0.5144118070602417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3060/3060 [00:11<00:00, 275.82it/s]\n",
      "100%|██████████| 3060/3060 [00:04<00:00, 695.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5: 0.5187106132507324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3060/3060 [00:11<00:00, 258.54it/s]\n",
      "100%|██████████| 3060/3060 [00:04<00:00, 701.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6: 0.5227249264717102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3060/3060 [00:12<00:00, 238.01it/s]\n",
      "100%|██████████| 3060/3060 [00:05<00:00, 605.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7: 0.5248480439186096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3060/3060 [00:12<00:00, 249.98it/s]\n",
      "100%|██████████| 3060/3060 [00:04<00:00, 645.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8: 0.5277913808822632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3060/3060 [00:11<00:00, 256.77it/s]\n",
      "100%|██████████| 3060/3060 [00:04<00:00, 659.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9: 0.5285513401031494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  model.train()\n",
    "  pbar = tqdm(total=len(dl))\n",
    "  for x_batch, y_batch in dl:\n",
    "    opt.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    y_pred = model(x_batch)\n",
    "    # Compute loss\n",
    "    loss = loss_fn(y_pred.view(-1,len(ds.vocabulary)),y_batch.view(-1))\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    opt.step()\n",
    "\n",
    "    pbar.update(1)\n",
    "  pbar.close()\n",
    "\n",
    "  model.eval()\n",
    "\n",
    "  metric.reset()\n",
    "  pbar = tqdm(total=len(dl))\n",
    "  for x_batch, y_batch in dl:\n",
    "    y_pred = model(x_batch)\n",
    "    metric(y_pred.view(-1,len(ds.vocabulary)),y_batch.view(-1))\n",
    "    pbar.update(1)\n",
    "  pbar.close()\n",
    "\n",
    "  acc = metric.compute().item()\n",
    "\n",
    "  print(f'epoch {epoch}: {acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Write a deterministic function to generate text given some starter text.  The function should iteratively add characters to the prompt using the trained model.  This version should be deterministic, in that in always takes the most likely next character according to the model.\n",
    "\n",
    "Test the function by prompting it with the first 10 characters in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i\n",
      "\n",
      " from the world the stars the stars the stars the stars the stars the stars the stars the stars the stars the stars the stars the stars the stars the stars the stars the stars the stars the stars the stars the stars the stars the stars the stars the stars the stars the stars the stars the stars the stars the stars the stars the stars the stars the stars the stars the stars the stars the stars the stars the stars the stars the stars the stars the stars the stars the stars the stars the stars the stars the stars the stars the stars the stars the stars the stars the stars the stars the stars the stars the stars the stars the stars the stars the stars the stars the stars the stars the stars the stars the stars the stars the stars the stars the stars the stars the stars the stars the stars the stars the stars the stars the stars the stars the stars the stars the stars the stars the stars the stars the stars the stars the stars the stars the stars the stars the stars the stars the stars the stars \n"
     ]
    }
   ],
   "source": [
    "def generate_text_deterministic(model,prompt,num_to_generate=1000):\n",
    "    model.eval()\n",
    "    gen_text = prompt\n",
    "\n",
    "    with torch.no_grad(): # disable gradient computation\n",
    "        for _ in range(num_to_generate):\n",
    "            indices = [ds.vocabulary.index(c) for c in gen_text]\n",
    "            in_tensor = torch.tensor(indices).unsqueeze(0).to(device)\n",
    "\n",
    "            lg = model(in_tensor)\n",
    "            last_lg = lg[0,-1,:]\n",
    "\n",
    "            # get next character deterministically\n",
    "            next_index = torch.argmax(last_lg).item()\n",
    "            next_char = ds.index_to_char[next_index]\n",
    "            gen_text += next_char\n",
    "    return gen_text\n",
    "\n",
    "# get starter text on first 10 characters in dataset\n",
    "start_text = text[:10]\n",
    "gen_out = generate_text_deterministic(model,start_text)\n",
    "print(gen_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Write a stochastic version of the text generation function.  This one should use `torch.multinomial` to sample the next character.  Note that you will need to apply `torch.softmax` to convert the model output to probabilities.  (In my experience if you don't this you end up with a CUDA error and you end up needing to restart your kernel, so be careful!)\n",
    "\n",
    "Test the function by prompting it with the first 10 characters in the dataset, and run the generation multiple times to verify the stochastic behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i\n",
      "\n",
      " from thee, as hast mine earrory-day?\n",
      " which proust new;\n",
      " thry jecoudd love i do beauty be acquainted creal once,\n",
      " and prestation'd,\n",
      " by be thaurs i nou firer be when i draw,\n",
      " thy glamt be\n",
      " arguldant is thy loves the his chimes,\n",
      " that befores trust, thou me infantion detery that was for as bisthings and intly thankle go owner i uprive your weaking awad not such cruety:\n",
      " be kind,\n",
      " sumindn;\n",
      " or and thy fighs,\n",
      " and therebrean the strong thou that you truth\n",
      " withon thing awked for love's window'd free;\n",
      " useld mone,\n",
      " and of deyle auturn thou dot the blood doth gracious,\n",
      " when i hold heach tell\n",
      " which thy life decerses of more no rilties new conter with spirl'e thy bared of she do i can not soke eyes,\n",
      " tank yournst wint should true most pling thou form this plevoe then fowll love and purbed sprince throw of a moo's to me gumn most thy shows,\n",
      " and thou am a fair, what grief;\n",
      " but in this,--deven of him while one dot otterest be fire,\n",
      " a falls summor comments,\n",
      " orde, but in the saully,\n",
      " or and burge\n"
     ]
    }
   ],
   "source": [
    "def generate_text_stochastic(model,prompt,num_to_generate=1000):\n",
    "    model.eval()\n",
    "    gen_text = prompt\n",
    "\n",
    "    with torch.no_grad(): # disable gradient computation\n",
    "        for _ in range(num_to_generate):\n",
    "            indices = [ds.vocabulary.index(c) for c in gen_text]\n",
    "            in_tensor = torch.tensor(indices).unsqueeze(0).to(device)\n",
    "\n",
    "            lg = model(in_tensor)\n",
    "            last_lg = lg[0,-1,:]\n",
    "\n",
    "            # get next character stochastically\n",
    "            probs = F.softmax(last_lg,dim=-1)\n",
    "            next_index = torch.multinomial(probs,1).item()\n",
    "            next_char = ds.index_to_char[next_index]\n",
    "            gen_text += next_char\n",
    "    return gen_text\n",
    "\n",
    "# get starter text on first 10 characters in dataset\n",
    "start_text = text[:10]\n",
    "gen_out = generate_text_stochastic(model,start_text)\n",
    "print(gen_out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
