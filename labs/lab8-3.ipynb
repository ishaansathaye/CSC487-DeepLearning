{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 8.3 Text generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab you will finish building your RNN text generator.  I found that this code actually runs pretty quickly on my MacBook without GPU acceleration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "seq_len = 20\n",
    "hidden_size = 100\n",
    "batch_size = 32\n",
    "lr = 3e-4\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchmetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the code to download and prepare the sonnet dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘sonnets.txt’ already there; not retrieving.\n"
     ]
    }
   ],
   "source": [
    "!wget --no-clobber \"https://www.dropbox.com/scl/fi/7r68l64ijemidyb9lf80q/sonnets.txt?rlkey=udb47coatr2zbrk31hsfbr22y&dl=1\" -O sonnets.txt\n",
    "text = (open(\"sonnets.txt\").read())\n",
    "text = text.lower().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i\n",
      "\n",
      " from fairest creatures we desire increase,\n",
      " that thereby beauty's rose might never die,\n",
      " but as the riper should by time decease,\n",
      " his tender heir might bear his memory:\n",
      " but thou, contracted to thine own bright eyes,\n",
      " feed'st thy light's flame with self-substantial fuel,\n",
      " making a famine where abundance lies,\n",
      " thy self thy foe, to thy sweet self too cruel:\n",
      " thou that art now the world's fresh ornament,\n",
      " and only herald to the gaudy spring,\n",
      " within thine own bud buriest thy content,\n",
      " and tender churl mak'st waste in niggarding:\n",
      "   pity the world, or else this glutton be,\n",
      "   to eat the world's due, by the grave and thee.\n",
      "\n",
      " ii\n",
      "\n",
      " when forty winters shall besiege thy brow,\n",
      " and dig deep trenches in thy beauty's field,\n",
      " thy youth's proud livery so gazed on now,\n",
      " will be a tatter'd weed of small worth held:\n",
      " then being asked, where all thy beauty lies,\n",
      " where all the treasure of thy lusty days;\n",
      " to say, within thine own deep sunken eyes,\n",
      " were an all-eating shame, and thriftless praise.\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's my solution for the `CharacterDataset` class.\n",
    "\n",
    "Note that it returns an entire sequence of tokens for the target (unlike what we did on Monday where it only output a single token for the target.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharacterDataset(Dataset):\n",
    "  def __init__(self,text,seq_len=100,device='cpu'):\n",
    "    \"\"\"\n",
    "    Initialize a dataset using character tokenization.\n",
    "    Arguments:\n",
    "      text: a string containing the dataset\n",
    "      seq_len: sequence length provided by __getitem__\n",
    "      device: device for PyTorch tensors\n",
    "    \"\"\"\n",
    "    self.text = text\n",
    "    self.seq_len = seq_len\n",
    "    self.vocabulary = ''.join(sorted(list(set(text))))\n",
    "    self.index_to_char = {n:char for n, char in enumerate(self.vocabulary)}\n",
    "    self.char_to_index = {char:n for n, char in enumerate(self.vocabulary)}\n",
    "    self.device = device\n",
    "\n",
    "  def __len__(self):\n",
    "    \"\"\" Return the length of sequences in the dataset. \"\"\"\n",
    "    return len(self.text)-self.seq_len-1\n",
    "\n",
    "  def __getitem__(self,idx):\n",
    "    \"\"\" Return the input and target sequences starting at given index. \"\"\"\n",
    "\n",
    "    text = self.text[idx:idx+self.seq_len+1]\n",
    "    tokens = self.encode(text)\n",
    "\n",
    "    return torch.tensor(tokens[:-1],device=self.device),torch.tensor(tokens[1:],device=self.device)\n",
    "  \n",
    "  def encode(self,text):\n",
    "    \"\"\" Encode a string to a list of integer tokens. \"\"\"\n",
    "    return list(map(self.char_to_index.get,text))\n",
    "\n",
    "  def decode(self,tokens):\n",
    "    \"\"\" Decode a list of token integers into a string. \"\"\"\n",
    "    return ''.join(list(map(self.index_to_char.get,tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = CharacterDataset(text,seq_len=seq_len,device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[38,\n",
       " 20,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 17,\n",
       " 29,\n",
       " 26,\n",
       " 24,\n",
       " 1,\n",
       " 17,\n",
       " 12,\n",
       " 20,\n",
       " 29,\n",
       " 16,\n",
       " 30,\n",
       " 31,\n",
       " 1,\n",
       " 14,\n",
       " 29,\n",
       " 16,\n",
       " 12,\n",
       " 31,\n",
       " 32,\n",
       " 29,\n",
       " 16,\n",
       " 30,\n",
       " 1,\n",
       " 34,\n",
       " 16,\n",
       " 1,\n",
       " 15,\n",
       " 16,\n",
       " 30,\n",
       " 20,\n",
       " 29,\n",
       " 16,\n",
       " 1,\n",
       " 20,\n",
       " 25,\n",
       " 14,\n",
       " 29,\n",
       " 16,\n",
       " 12,\n",
       " 30,\n",
       " 16,\n",
       " 6,\n",
       " 0,\n",
       " 1,\n",
       " 31,\n",
       " 19,\n",
       " 12,\n",
       " 31,\n",
       " 1,\n",
       " 31,\n",
       " 19,\n",
       " 16,\n",
       " 29,\n",
       " 16,\n",
       " 13,\n",
       " 36,\n",
       " 1,\n",
       " 13,\n",
       " 16,\n",
       " 12,\n",
       " 32,\n",
       " 31,\n",
       " 36,\n",
       " 3,\n",
       " 30,\n",
       " 1,\n",
       " 29,\n",
       " 26,\n",
       " 30,\n",
       " 16,\n",
       " 1,\n",
       " 24,\n",
       " 20,\n",
       " 18,\n",
       " 19,\n",
       " 31,\n",
       " 1,\n",
       " 25,\n",
       " 16,\n",
       " 33,\n",
       " 16,\n",
       " 29,\n",
       " 1,\n",
       " 15,\n",
       " 20,\n",
       " 16,\n",
       " 6,\n",
       " 0,\n",
       " 1,\n",
       " 13,\n",
       " 32,\n",
       " 31,\n",
       " 1,\n",
       " 12,\n",
       " 30]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.encode(text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i\n",
      "\n",
      " from fairest creatures we desire increase,\n",
      " that thereby beauty's rose might never die,\n",
      " but as\n"
     ]
    }
   ],
   "source": [
    "print(ds.decode(ds.encode(text[:100])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([20]), torch.Size([20]))"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = ds[0]\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(ds,shuffle=True,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's my solution for the recurrent neural network (RNN) implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharacterRNN(nn.Module):\n",
    "  def __init__(self,vocabulary_size,hidden_size):\n",
    "    super().__init__()\n",
    "    self.embedding = nn.Embedding(vocabulary_size,hidden_size)\n",
    "    self.hidden_size = hidden_size\n",
    "    self.U = nn.Linear(hidden_size,hidden_size)\n",
    "    self.W = nn.Linear(hidden_size,hidden_size)\n",
    "    self.act = nn.SiLU()\n",
    "    self.V = nn.Linear(hidden_size,vocabulary_size)\n",
    "\n",
    "  def forward(self,x):\n",
    "    x = self.embedding(x)\n",
    "    B,N = x.shape[:2]\n",
    "    h = torch.zeros(B,self.hidden_size).to(x.device)\n",
    "    Ux = self.U(x)\n",
    "    y = []\n",
    "    for i in range(N):\n",
    "      Wh = self.W(h)\n",
    "      h = self.act(Ux[:,i] + Wh)\n",
    "      y.append(self.V(h))\n",
    "    return torch.stack(y,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CharacterRNN(len(ds.vocabulary),hidden_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 20]), torch.Size([32, 20]))"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_batch, y_batch = next(iter(dl))\n",
    "x_batch.shape, y_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 20, 39])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x_batch).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally here is my code to train the model.\n",
    "\n",
    "Note that I needed to use `.view()` to reshape the model output and target, becuase the loss and metric functions want the data to have shape [B,C] not [B,N,C]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MulticlassAccuracy()"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt = torch.optim.Adam(model.parameters(),lr=lr)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "metric = torchmetrics.classification.Accuracy(task=\"multiclass\", num_classes=len(ds.vocabulary))\n",
    "metric.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 1844/3060 [00:57<00:37, 32.09it/s]\n",
      "100%|██████████| 3060/3060 [00:13<00:00, 232.18it/s]\n",
      "100%|██████████| 3060/3060 [00:04<00:00, 654.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: 0.4693010151386261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3060/3060 [00:12<00:00, 245.33it/s]\n",
      "100%|██████████| 3060/3060 [00:04<00:00, 621.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: 0.49240797758102417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3060/3060 [00:14<00:00, 214.40it/s]\n",
      "100%|██████████| 3060/3060 [00:04<00:00, 628.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2: 0.5034581422805786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3060/3060 [00:11<00:00, 257.09it/s]\n",
      "100%|██████████| 3060/3060 [00:04<00:00, 671.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3: 0.5094745755195618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3060/3060 [00:13<00:00, 231.79it/s]\n",
      "100%|██████████| 3060/3060 [00:04<00:00, 649.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4: 0.5142049193382263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3060/3060 [00:13<00:00, 224.68it/s]\n",
      "100%|██████████| 3060/3060 [00:04<00:00, 625.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5: 0.5181493163108826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3060/3060 [00:15<00:00, 194.47it/s]\n",
      " 38%|███▊      | 1160/3060 [00:01<00:03, 594.91it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[105], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m pbar \u001b[38;5;241m=\u001b[39m tqdm(total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(dl))\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x_batch, y_batch \u001b[38;5;129;01min\u001b[39;00m dl:\n\u001b[0;32m---> 26\u001b[0m   y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m   metric(y_pred\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;28mlen\u001b[39m(ds\u001b[38;5;241m.\u001b[39mvocabulary)),y_batch\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     28\u001b[0m   pbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Git/CalPolyCourses/CSC487-DeepLearning/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Git/CalPolyCourses/CSC487-DeepLearning/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[100], line 18\u001b[0m, in \u001b[0;36mCharacterRNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     16\u001b[0m y \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N):\n\u001b[0;32m---> 18\u001b[0m   Wh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mW\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m   h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(Ux[:,i] \u001b[38;5;241m+\u001b[39m Wh)\n\u001b[1;32m     20\u001b[0m   y\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mV(h))\n",
      "File \u001b[0;32m~/Git/CalPolyCourses/CSC487-DeepLearning/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Git/CalPolyCourses/CSC487-DeepLearning/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Git/CalPolyCourses/CSC487-DeepLearning/env/lib/python3.10/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  model.train()\n",
    "  pbar = tqdm(total=len(dl))\n",
    "  for x_batch, y_batch in dl:\n",
    "    opt.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    y_pred = model(x_batch)\n",
    "    # Compute loss\n",
    "    loss = loss_fn(y_pred.view(-1,len(ds.vocabulary)),y_batch.view(-1))\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    opt.step()\n",
    "\n",
    "    pbar.update(1)\n",
    "  pbar.close()\n",
    "\n",
    "  model.eval()\n",
    "\n",
    "  metric.reset()\n",
    "  pbar = tqdm(total=len(dl))\n",
    "  for x_batch, y_batch in dl:\n",
    "    y_pred = model(x_batch)\n",
    "    metric(y_pred.view(-1,len(ds.vocabulary)),y_batch.view(-1))\n",
    "    pbar.update(1)\n",
    "  pbar.close()\n",
    "\n",
    "  acc = metric.compute().item()\n",
    "\n",
    "  print(f'epoch {epoch}: {acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Write a deterministic function to generate text given some starter text.  The function should iteratively add characters to the prompt using the trained model.  This version should be deterministic, in that in always takes the most likely next character according to the model.\n",
    "\n",
    "Test the function by prompting it with the first 10 characters in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_deterministic(model,prompt,num_to_generate=1000):\n",
    "    model.eval()\n",
    "    gen_text = prompt\n",
    "\n",
    "    with torch.no_grad(): # disable gradient computation\n",
    "        for _ in range(num_to_generate):\n",
    "            indices = [ds.vocabulary.index(c) for c in gen_text]\n",
    "            in_tensor = torch.tensor(indices).unsqueeze(0).to(device)\n",
    "\n",
    "            lg = model(in_tensor)\n",
    "            last_lg = lg[0,-1,:]\n",
    "\n",
    "            # get next character deterministically\n",
    "            next_index = torch.argmax(last_lg).item()\n",
    "            next_char = ds.index_to_char[next_index]\n",
    "            gen_text += next_char\n",
    "    return gen_text\n",
    "\n",
    "# get starter text on first 10 characters in dataset\n",
    "start_text = text[:10]\n",
    "gen_out = generate_text_deterministic(model,start_text)\n",
    "print(gen_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Write a stochastic version of the text generation function.  This one should use `torch.multinomial` to sample the next character.  Note that you will need to apply `torch.softmax` to convert the model output to probabilities.  (In my experience if you don't this you end up with a CUDA error and you end up needing to restart your kernel, so be careful!)\n",
    "\n",
    "Test the function by prompting it with the first 10 characters in the dataset, and run the generation multiple times to verify the stochastic behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_stochastic(model,prompt,num_to_generate=1000):\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
