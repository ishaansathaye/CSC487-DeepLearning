{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 3.1: Basic Neural Network in PyTorch - Solution\n",
    "\n",
    "Let's create a linear classifier one more time, but using PyTorch's automatic differentiation and optimization algorithms.  Then you will extend the perceptron into a multi-layer perceptron (MLP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to explicitly tell PyTorch when creating a tensor that we are interested in later computing its gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5., requires_grad=True)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor(5.,requires_grad=True)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(28., grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.tensor(6.,requires_grad=True)\n",
    "c = 2*a+3*b\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extract the gradients, we first need to call `backward()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to get the gradient of any variable with respect to `c`, we simply access the `grad` attribute of that variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load and format the Palmer penguins dataset for multi-class classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from palmerpenguins import load_penguins\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_penguins()\n",
    "\n",
    "# drop rows with missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# get two features\n",
    "X = df[['flipper_length_mm','bill_length_mm']].values\n",
    "\n",
    "# convert species labels to integers\n",
    "y = df['species'].map({'Adelie':0,'Chinstrap':1,'Gentoo':2}).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the learning algorithm work more smoothly, we we will subtract the mean of each feature.\n",
    "\n",
    "Here `np.mean` calculates a mean, and `axis=0` tells NumPy to calculate the mean over the rows (calculate the mean of each column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X -= np.mean(X,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will convert our `X` and `y` arrays to torch Tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(X).float()\n",
    "y = torch.tensor(y).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `torch.nn.Sequential` class creates a feed-forward network from a list of `nn.Module` objects.  Here we provide a single `nn.Linear` class which performs an affine transformation ($Wx+b$) so that we will have a linear classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(2,3), # two inputs, three outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a cross-entropy loss function object and a stochastic gradient descent (SGD) optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-2\n",
    "opt = torch.optim.SGD(linear_model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can iteratively optimize the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: loss is 5.746509075164795\n",
      "epoch 1: loss is 3.662224769592285\n",
      "epoch 2: loss is 2.4052345752716064\n",
      "epoch 3: loss is 1.9485793113708496\n",
      "epoch 4: loss is 1.722406029701233\n",
      "epoch 5: loss is 1.5718508958816528\n",
      "epoch 6: loss is 1.458114743232727\n",
      "epoch 7: loss is 1.3650604486465454\n",
      "epoch 8: loss is 1.2849527597427368\n",
      "epoch 9: loss is 1.2136406898498535\n",
      "epoch 10: loss is 1.1486711502075195\n",
      "epoch 11: loss is 1.0885024070739746\n",
      "epoch 12: loss is 1.0321266651153564\n",
      "epoch 13: loss is 0.9788690209388733\n",
      "epoch 14: loss is 0.92827308177948\n",
      "epoch 15: loss is 0.8800314664840698\n",
      "epoch 16: loss is 0.8339424133300781\n",
      "epoch 17: loss is 0.7898805141448975\n",
      "epoch 18: loss is 0.7477768659591675\n",
      "epoch 19: loss is 0.7076043486595154\n",
      "epoch 20: loss is 0.6693655252456665\n",
      "epoch 21: loss is 0.6330825090408325\n",
      "epoch 22: loss is 0.5987884998321533\n",
      "epoch 23: loss is 0.5665190815925598\n",
      "epoch 24: loss is 0.5363045334815979\n",
      "epoch 25: loss is 0.5081632137298584\n",
      "epoch 26: loss is 0.48209652304649353\n",
      "epoch 27: loss is 0.45808494091033936\n",
      "epoch 28: loss is 0.43608564138412476\n",
      "epoch 29: loss is 0.4160314202308655\n",
      "epoch 30: loss is 0.3978307545185089\n",
      "epoch 31: loss is 0.38136905431747437\n",
      "epoch 32: loss is 0.3665127754211426\n",
      "epoch 33: loss is 0.35311567783355713\n",
      "epoch 34: loss is 0.34102675318717957\n",
      "epoch 35: loss is 0.3300987780094147\n",
      "epoch 36: loss is 0.32019397616386414\n",
      "epoch 37: loss is 0.3111879825592041\n",
      "epoch 38: loss is 0.30297109484672546\n",
      "epoch 39: loss is 0.29544776678085327\n",
      "epoch 40: loss is 0.28853583335876465\n",
      "epoch 41: loss is 0.28216442465782166\n",
      "epoch 42: loss is 0.27627289295196533\n",
      "epoch 43: loss is 0.2708088457584381\n",
      "epoch 44: loss is 0.26572704315185547\n",
      "epoch 45: loss is 0.26098835468292236\n",
      "epoch 46: loss is 0.2565585970878601\n",
      "epoch 47: loss is 0.2524082064628601\n",
      "epoch 48: loss is 0.24851098656654358\n",
      "epoch 49: loss is 0.244843989610672\n",
      "epoch 50: loss is 0.24138697981834412\n",
      "epoch 51: loss is 0.23812204599380493\n",
      "epoch 52: loss is 0.23503318428993225\n",
      "epoch 53: loss is 0.23210613429546356\n",
      "epoch 54: loss is 0.22932817041873932\n",
      "epoch 55: loss is 0.22668786346912384\n",
      "epoch 56: loss is 0.22417500615119934\n",
      "epoch 57: loss is 0.2217802256345749\n",
      "epoch 58: loss is 0.21949516236782074\n",
      "epoch 59: loss is 0.21731218695640564\n",
      "epoch 60: loss is 0.2152244597673416\n",
      "epoch 61: loss is 0.2132255882024765\n",
      "epoch 62: loss is 0.21130993962287903\n",
      "epoch 63: loss is 0.2094721794128418\n",
      "epoch 64: loss is 0.20770753920078278\n",
      "epoch 65: loss is 0.2060116082429886\n",
      "epoch 66: loss is 0.20438027381896973\n",
      "epoch 67: loss is 0.20280984044075012\n",
      "epoch 68: loss is 0.20129679143428802\n",
      "epoch 69: loss is 0.19983793795108795\n",
      "epoch 70: loss is 0.19843044877052307\n",
      "epoch 71: loss is 0.1970714032649994\n",
      "epoch 72: loss is 0.1957583725452423\n",
      "epoch 73: loss is 0.19448894262313843\n",
      "epoch 74: loss is 0.19326087832450867\n",
      "epoch 75: loss is 0.19207221269607544\n",
      "epoch 76: loss is 0.19092096388339996\n",
      "epoch 77: loss is 0.18980534374713898\n",
      "epoch 78: loss is 0.18872368335723877\n",
      "epoch 79: loss is 0.187674418091774\n",
      "epoch 80: loss is 0.18665601313114166\n",
      "epoch 81: loss is 0.18566718697547913\n",
      "epoch 82: loss is 0.18470652401447296\n",
      "epoch 83: loss is 0.18377292156219482\n",
      "epoch 84: loss is 0.18286511301994324\n",
      "epoch 85: loss is 0.1819821000099182\n",
      "epoch 86: loss is 0.1811227649450302\n",
      "epoch 87: loss is 0.1802862137556076\n",
      "epoch 88: loss is 0.17947152256965637\n",
      "epoch 89: loss is 0.1786777675151825\n",
      "epoch 90: loss is 0.1779041886329651\n",
      "epoch 91: loss is 0.17715001106262207\n",
      "epoch 92: loss is 0.17641445994377136\n",
      "epoch 93: loss is 0.17569682002067566\n",
      "epoch 94: loss is 0.1749965101480484\n",
      "epoch 95: loss is 0.17431281507015228\n",
      "epoch 96: loss is 0.17364513874053955\n",
      "epoch 97: loss is 0.172993004322052\n",
      "epoch 98: loss is 0.1723557859659195\n",
      "epoch 99: loss is 0.17173294723033905\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    opt.zero_grad() # zero out the gradients\n",
    "\n",
    "    z = linear_model(X) # compute z values\n",
    "    loss = loss_fn(z,y) # compute loss\n",
    "\n",
    "    loss.backward() # compute gradients\n",
    "\n",
    "    opt.step() # apply gradients\n",
    "\n",
    "    print(f'epoch {epoch}: loss is {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "Extend the above code to implement an MLP with a single hidden layer of size 100.\n",
    "\n",
    "Write code to compute the accuracy of each model.\n",
    "\n",
    "Can you get the MLP to outperform the linear model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: loss is 1.5085662603378296\n",
      "epoch 1: loss is 0.5871738791465759\n",
      "epoch 2: loss is 0.4000960886478424\n",
      "epoch 3: loss is 0.322466105222702\n",
      "epoch 4: loss is 0.27651047706604004\n",
      "epoch 5: loss is 0.2495923489332199\n",
      "epoch 6: loss is 0.23229750990867615\n",
      "epoch 7: loss is 0.22056759893894196\n",
      "epoch 8: loss is 0.21197642385959625\n",
      "epoch 9: loss is 0.2052653729915619\n",
      "epoch 10: loss is 0.19975292682647705\n",
      "epoch 11: loss is 0.19506588578224182\n",
      "epoch 12: loss is 0.1909896433353424\n",
      "epoch 13: loss is 0.18738973140716553\n",
      "epoch 14: loss is 0.18417416512966156\n",
      "epoch 15: loss is 0.18127582967281342\n",
      "epoch 16: loss is 0.17864347994327545\n",
      "epoch 17: loss is 0.1762370616197586\n",
      "epoch 18: loss is 0.1740245372056961\n",
      "epoch 19: loss is 0.17197997868061066\n",
      "epoch 20: loss is 0.1700822412967682\n",
      "epoch 21: loss is 0.16831357777118683\n",
      "epoch 22: loss is 0.16665948927402496\n",
      "epoch 23: loss is 0.16510747373104095\n",
      "epoch 24: loss is 0.1636471152305603\n",
      "epoch 25: loss is 0.16226951777935028\n",
      "epoch 26: loss is 0.1609668880701065\n",
      "epoch 27: loss is 0.1597326248884201\n",
      "epoch 28: loss is 0.15856097638607025\n",
      "epoch 29: loss is 0.15744686126708984\n",
      "epoch 30: loss is 0.1563858836889267\n",
      "epoch 31: loss is 0.1553741693496704\n",
      "epoch 32: loss is 0.15440824627876282\n",
      "epoch 33: loss is 0.15348507463932037\n",
      "epoch 34: loss is 0.15260189771652222\n",
      "epoch 35: loss is 0.15175625681877136\n",
      "epoch 36: loss is 0.15094594657421112\n",
      "epoch 37: loss is 0.1501690000295639\n",
      "epoch 38: loss is 0.14942355453968048\n",
      "epoch 39: loss is 0.1487080156803131\n",
      "epoch 40: loss is 0.14802080392837524\n",
      "epoch 41: loss is 0.14736059308052063\n",
      "epoch 42: loss is 0.14672605693340302\n",
      "epoch 43: loss is 0.1461159735918045\n",
      "epoch 44: loss is 0.14552925527095795\n",
      "epoch 45: loss is 0.14496488869190216\n",
      "epoch 46: loss is 0.1444218009710312\n",
      "epoch 47: loss is 0.14389914274215698\n",
      "epoch 48: loss is 0.1433960199356079\n",
      "epoch 49: loss is 0.14291156828403473\n",
      "epoch 50: loss is 0.14244501292705536\n",
      "epoch 51: loss is 0.14199557900428772\n",
      "epoch 52: loss is 0.1415625363588333\n",
      "epoch 53: loss is 0.14114515483379364\n",
      "epoch 54: loss is 0.14074279367923737\n",
      "epoch 55: loss is 0.14035478234291077\n",
      "epoch 56: loss is 0.1399804800748825\n",
      "epoch 57: loss is 0.13961932063102722\n",
      "epoch 58: loss is 0.13927064836025238\n",
      "epoch 59: loss is 0.1389339566230774\n",
      "epoch 60: loss is 0.13860870897769928\n",
      "epoch 61: loss is 0.13829436898231506\n",
      "epoch 62: loss is 0.13799044489860535\n",
      "epoch 63: loss is 0.13769637048244476\n",
      "epoch 64: loss is 0.13741180300712585\n",
      "epoch 65: loss is 0.13713619112968445\n",
      "epoch 66: loss is 0.1368691623210907\n",
      "epoch 67: loss is 0.1366102546453476\n",
      "epoch 68: loss is 0.13635915517807007\n",
      "epoch 69: loss is 0.1361154019832611\n",
      "epoch 70: loss is 0.13587872684001923\n",
      "epoch 71: loss is 0.135648712515831\n",
      "epoch 72: loss is 0.13542510569095612\n",
      "epoch 73: loss is 0.13520753383636475\n",
      "epoch 74: loss is 0.13499577343463898\n",
      "epoch 75: loss is 0.13478946685791016\n",
      "epoch 76: loss is 0.13458843529224396\n",
      "epoch 77: loss is 0.1343923956155777\n",
      "epoch 78: loss is 0.13420109450817108\n",
      "epoch 79: loss is 0.13401436805725098\n",
      "epoch 80: loss is 0.1338319629430771\n",
      "epoch 81: loss is 0.1336536854505539\n",
      "epoch 82: loss is 0.13347937166690826\n",
      "epoch 83: loss is 0.13330887258052826\n",
      "epoch 84: loss is 0.13314196467399597\n",
      "epoch 85: loss is 0.13297852873802185\n",
      "epoch 86: loss is 0.13281841576099396\n",
      "epoch 87: loss is 0.13266147673130035\n",
      "epoch 88: loss is 0.1325075924396515\n",
      "epoch 89: loss is 0.1323566436767578\n",
      "epoch 90: loss is 0.13220854103565216\n",
      "epoch 91: loss is 0.1320631355047226\n",
      "epoch 92: loss is 0.13192033767700195\n",
      "epoch 93: loss is 0.13178005814552307\n",
      "epoch 94: loss is 0.1316421926021576\n",
      "epoch 95: loss is 0.13150666654109955\n",
      "epoch 96: loss is 0.13137340545654297\n",
      "epoch 97: loss is 0.1312423050403595\n",
      "epoch 98: loss is 0.13111332058906555\n",
      "epoch 99: loss is 0.13098637759685516\n"
     ]
    }
   ],
   "source": [
    "mlp = torch.nn.Sequential(\n",
    "    torch.nn.Linear(2,100),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(100,3),\n",
    ")\n",
    "\n",
    "opt = torch.optim.SGD(mlp.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    opt.zero_grad() # zero out the gradients\n",
    "\n",
    "    z = mlp(X) # compute z values\n",
    "    loss = loss_fn(z,y) # compute loss\n",
    "\n",
    "    loss.backward() # compute gradients\n",
    "\n",
    "    opt.step() # apply gradients\n",
    "\n",
    "    print(f'epoch {epoch}: loss is {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear model accuracy: 0.9489489197731018\n",
      "mlp accuracy: 0.9519519805908203\n"
     ]
    }
   ],
   "source": [
    "# Compute the accuracy of each model\n",
    "\n",
    "def accuracy(y_pred,y_true):\n",
    "    return (y_pred == y_true).float().mean()\n",
    "\n",
    "y_pred = linear_model(X).argmax(dim=1)\n",
    "acc = accuracy(y_pred,y)\n",
    "print(f'linear model accuracy: {acc.item()}')\n",
    "\n",
    "y_pred = mlp(X).argmax(dim=1)\n",
    "acc = accuracy(y_pred,y)\n",
    "print(f'mlp accuracy: {acc.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP has outperformed the linear model with it having an accuracy of 0.952 while the linear model accuracy is 0.949."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
