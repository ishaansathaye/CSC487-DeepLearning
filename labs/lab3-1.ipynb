{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 3.1: Basic Neural Network in PyTorch - Solution\n",
    "\n",
    "Let's create a linear classifier one more time, but using PyTorch's automatic differentiation and optimization algorithms.  Then you will extend the perceptron into a multi-layer perceptron (MLP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to explicitly tell PyTorch when creating a tensor that we are interested in later computing its gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5., requires_grad=True)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor(5.,requires_grad=True)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(28., grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.tensor(6.,requires_grad=True)\n",
    "c = 2*a+3*b\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extract the gradients, we first need to call `backward()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to get the gradient of any variable with respect to `c`, we simply access the `grad` attribute of that variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load and format the Palmer penguins dataset for multi-class classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from palmerpenguins import load_penguins\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_penguins()\n",
    "\n",
    "# drop rows with missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# get two features\n",
    "X = df[['flipper_length_mm','bill_length_mm']].values\n",
    "\n",
    "# convert species labels to integers\n",
    "y = df['species'].map({'Adelie':0,'Chinstrap':1,'Gentoo':2}).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the learning algorithm work more smoothly, we we will subtract the mean of each feature.\n",
    "\n",
    "Here `np.mean` calculates a mean, and `axis=0` tells NumPy to calculate the mean over the rows (calculate the mean of each column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X -= np.mean(X,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will convert our `X` and `y` arrays to torch Tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(X).float()\n",
    "y = torch.tensor(y).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `torch.nn.Sequential` class creates a feed-forward network from a list of `nn.Module` objects.  Here we provide a single `nn.Linear` class which performs an affine transformation ($Wx+b$) so that we will have a linear classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(2,3), # two inputs, three outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a cross-entropy loss function object and a stochastic gradient descent (SGD) optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-2\n",
    "opt = torch.optim.SGD(linear_model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can iteratively optimize the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: loss is 0.5305097699165344\n",
      "epoch 1: loss is 0.5094084143638611\n",
      "epoch 2: loss is 0.4900461435317993\n",
      "epoch 3: loss is 0.4723173975944519\n",
      "epoch 4: loss is 0.45609423518180847\n",
      "epoch 5: loss is 0.44124436378479004\n",
      "epoch 6: loss is 0.42764177918434143\n",
      "epoch 7: loss is 0.415169894695282\n",
      "epoch 8: loss is 0.4037216305732727\n",
      "epoch 9: loss is 0.39319851994514465\n",
      "epoch 10: loss is 0.38350945711135864\n",
      "epoch 11: loss is 0.3745700418949127\n",
      "epoch 12: loss is 0.36630210280418396\n",
      "epoch 13: loss is 0.3586336076259613\n",
      "epoch 14: loss is 0.35149848461151123\n",
      "epoch 15: loss is 0.3448368012905121\n",
      "epoch 16: loss is 0.3385946452617645\n",
      "epoch 17: loss is 0.33272409439086914\n",
      "epoch 18: loss is 0.3271826505661011\n",
      "epoch 19: loss is 0.3219333291053772\n",
      "epoch 20: loss is 0.31694409251213074\n",
      "epoch 21: loss is 0.312187135219574\n",
      "epoch 22: loss is 0.307638943195343\n",
      "epoch 23: loss is 0.3032791018486023\n",
      "epoch 24: loss is 0.29909050464630127\n",
      "epoch 25: loss is 0.2950584888458252\n",
      "epoch 26: loss is 0.29117056727409363\n",
      "epoch 27: loss is 0.28741616010665894\n",
      "epoch 28: loss is 0.2837861478328705\n",
      "epoch 29: loss is 0.28027278184890747\n",
      "epoch 30: loss is 0.27686935663223267\n",
      "epoch 31: loss is 0.2735700011253357\n",
      "epoch 32: loss is 0.27036964893341064\n",
      "epoch 33: loss is 0.2672637701034546\n",
      "epoch 34: loss is 0.26424843072891235\n",
      "epoch 35: loss is 0.2613200545310974\n",
      "epoch 36: loss is 0.2584754228591919\n",
      "epoch 37: loss is 0.25571155548095703\n",
      "epoch 38: loss is 0.2530257999897003\n",
      "epoch 39: loss is 0.2504156827926636\n",
      "epoch 40: loss is 0.2478787750005722\n",
      "epoch 41: loss is 0.24541299045085907\n",
      "epoch 42: loss is 0.24301621317863464\n",
      "epoch 43: loss is 0.24068652093410492\n",
      "epoch 44: loss is 0.23842191696166992\n",
      "epoch 45: loss is 0.23622071743011475\n",
      "epoch 46: loss is 0.23408114910125732\n",
      "epoch 47: loss is 0.2320014089345932\n",
      "epoch 48: loss is 0.22997991740703583\n",
      "epoch 49: loss is 0.22801506519317627\n",
      "epoch 50: loss is 0.22610528767108917\n",
      "epoch 51: loss is 0.224249005317688\n",
      "epoch 52: loss is 0.22244471311569214\n",
      "epoch 53: loss is 0.2206910103559494\n",
      "epoch 54: loss is 0.21898633241653442\n",
      "epoch 55: loss is 0.21732932329177856\n",
      "epoch 56: loss is 0.2157185673713684\n",
      "epoch 57: loss is 0.21415270864963531\n",
      "epoch 58: loss is 0.2126304656267166\n",
      "epoch 59: loss is 0.21115045249462128\n",
      "epoch 60: loss is 0.2097114771604538\n",
      "epoch 61: loss is 0.2083122283220291\n",
      "epoch 62: loss is 0.20695151388645172\n",
      "epoch 63: loss is 0.20562812685966492\n",
      "epoch 64: loss is 0.20434091985225677\n",
      "epoch 65: loss is 0.20308880507946014\n",
      "epoch 66: loss is 0.20187066495418549\n",
      "epoch 67: loss is 0.20068545639514923\n",
      "epoch 68: loss is 0.19953206181526184\n",
      "epoch 69: loss is 0.19840964674949646\n",
      "epoch 70: loss is 0.19731712341308594\n",
      "epoch 71: loss is 0.19625356793403625\n",
      "epoch 72: loss is 0.19521810114383698\n",
      "epoch 73: loss is 0.19420985877513885\n",
      "epoch 74: loss is 0.19322800636291504\n",
      "epoch 75: loss is 0.1922716498374939\n",
      "epoch 76: loss is 0.19134004414081573\n",
      "epoch 77: loss is 0.19043247401714325\n",
      "epoch 78: loss is 0.1895480751991272\n",
      "epoch 79: loss is 0.1886862814426422\n",
      "epoch 80: loss is 0.1878463476896286\n",
      "epoch 81: loss is 0.18702760338783264\n",
      "epoch 82: loss is 0.18622943758964539\n",
      "epoch 83: loss is 0.1854511797428131\n",
      "epoch 84: loss is 0.18469230830669403\n",
      "epoch 85: loss is 0.18395216763019562\n",
      "epoch 86: loss is 0.18323029577732086\n",
      "epoch 87: loss is 0.1825261116027832\n",
      "epoch 88: loss is 0.18183907866477966\n",
      "epoch 89: loss is 0.18116872012615204\n",
      "epoch 90: loss is 0.18051457405090332\n",
      "epoch 91: loss is 0.17987613379955292\n",
      "epoch 92: loss is 0.1792529821395874\n",
      "epoch 93: loss is 0.17864465713500977\n",
      "epoch 94: loss is 0.17805075645446777\n",
      "epoch 95: loss is 0.177470862865448\n",
      "epoch 96: loss is 0.1769045740365982\n",
      "epoch 97: loss is 0.17635153234004974\n",
      "epoch 98: loss is 0.17581135034561157\n",
      "epoch 99: loss is 0.17528365552425385\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    opt.zero_grad() # zero out the gradients\n",
    "\n",
    "    z = linear_model(X) # compute z values\n",
    "    loss = loss_fn(z,y) # compute loss\n",
    "\n",
    "    loss.backward() # compute gradients\n",
    "\n",
    "    opt.step() # apply gradients\n",
    "\n",
    "    print(f'epoch {epoch}: loss is {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "Extend the above code to implement an MLP with a single hidden layer of size 100.\n",
    "\n",
    "Write code to compute the accuracy of each model.\n",
    "\n",
    "Can you get the MLP to outperform the linear model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: loss is 4.431340217590332\n",
      "epoch 1: loss is 0.4677637815475464\n",
      "epoch 2: loss is 0.2965518534183502\n",
      "epoch 3: loss is 0.2327469438314438\n",
      "epoch 4: loss is 0.2164786159992218\n",
      "epoch 5: loss is 0.2075054794549942\n",
      "epoch 6: loss is 0.2003447413444519\n",
      "epoch 7: loss is 0.19435431063175201\n",
      "epoch 8: loss is 0.1891857236623764\n",
      "epoch 9: loss is 0.18464040756225586\n",
      "epoch 10: loss is 0.18059131503105164\n",
      "epoch 11: loss is 0.17695100605487823\n",
      "epoch 12: loss is 0.17365314066410065\n",
      "epoch 13: loss is 0.17064611613750458\n",
      "epoch 14: loss is 0.1678888201713562\n",
      "epoch 15: loss is 0.16534817218780518\n",
      "epoch 16: loss is 0.16299712657928467\n",
      "epoch 17: loss is 0.1608133614063263\n",
      "epoch 18: loss is 0.15877841413021088\n",
      "epoch 19: loss is 0.1568765938282013\n",
      "epoch 20: loss is 0.15509499609470367\n",
      "epoch 21: loss is 0.15342234075069427\n",
      "epoch 22: loss is 0.15184913575649261\n",
      "epoch 23: loss is 0.15036706626415253\n",
      "epoch 24: loss is 0.1489691585302353\n",
      "epoch 25: loss is 0.14764906466007233\n",
      "epoch 26: loss is 0.14640140533447266\n",
      "epoch 27: loss is 0.14522133767604828\n",
      "epoch 28: loss is 0.14410455524921417\n",
      "epoch 29: loss is 0.14304722845554352\n",
      "epoch 30: loss is 0.14204582571983337\n",
      "epoch 31: loss is 0.14109723269939423\n",
      "epoch 32: loss is 0.14020006358623505\n",
      "epoch 33: loss is 0.13935062289237976\n",
      "epoch 34: loss is 0.13854576647281647\n",
      "epoch 35: loss is 0.13778308033943176\n",
      "epoch 36: loss is 0.13706032931804657\n",
      "epoch 37: loss is 0.13637541234493256\n",
      "epoch 38: loss is 0.13572630286216736\n",
      "epoch 39: loss is 0.1351109743118286\n",
      "epoch 40: loss is 0.13452762365341187\n",
      "epoch 41: loss is 0.13397440314292908\n",
      "epoch 42: loss is 0.13344961404800415\n",
      "epoch 43: loss is 0.1329515427350998\n",
      "epoch 44: loss is 0.13247860968112946\n",
      "epoch 45: loss is 0.13202925026416779\n",
      "epoch 46: loss is 0.13160203397274017\n",
      "epoch 47: loss is 0.1311955600976944\n",
      "epoch 48: loss is 0.1308085322380066\n",
      "epoch 49: loss is 0.1304396241903305\n",
      "epoch 50: loss is 0.1300876885652542\n",
      "epoch 51: loss is 0.12975159287452698\n",
      "epoch 52: loss is 0.12943026423454285\n",
      "epoch 53: loss is 0.12912271916866302\n",
      "epoch 54: loss is 0.12882809340953827\n",
      "epoch 55: loss is 0.12854544818401337\n",
      "epoch 56: loss is 0.12827402353286743\n",
      "epoch 57: loss is 0.12801313400268555\n",
      "epoch 58: loss is 0.1277620494365692\n",
      "epoch 59: loss is 0.1275200992822647\n",
      "epoch 60: loss is 0.12728679180145264\n",
      "epoch 61: loss is 0.12706153094768524\n",
      "epoch 62: loss is 0.12684382498264313\n",
      "epoch 63: loss is 0.12663325667381287\n",
      "epoch 64: loss is 0.12642936408519745\n",
      "epoch 65: loss is 0.12623178958892822\n",
      "epoch 66: loss is 0.12604016065597534\n",
      "epoch 67: loss is 0.12585419416427612\n",
      "epoch 68: loss is 0.12567353248596191\n",
      "epoch 69: loss is 0.12549792230129242\n",
      "epoch 70: loss is 0.12532711029052734\n",
      "epoch 71: loss is 0.12516088783740997\n",
      "epoch 72: loss is 0.12499897927045822\n",
      "epoch 73: loss is 0.12484123557806015\n",
      "epoch 74: loss is 0.12468745559453964\n",
      "epoch 75: loss is 0.12453743815422058\n",
      "epoch 76: loss is 0.12439103424549103\n",
      "epoch 77: loss is 0.12424809485673904\n",
      "epoch 78: loss is 0.1241084486246109\n",
      "epoch 79: loss is 0.12397199869155884\n",
      "epoch 80: loss is 0.12383858114480972\n",
      "epoch 81: loss is 0.1237080991268158\n",
      "epoch 82: loss is 0.1235804483294487\n",
      "epoch 83: loss is 0.12345550954341888\n",
      "epoch 84: loss is 0.1233331635594368\n",
      "epoch 85: loss is 0.12321334332227707\n",
      "epoch 86: loss is 0.12309595942497253\n",
      "epoch 87: loss is 0.12298092246055603\n",
      "epoch 88: loss is 0.12286815792322159\n",
      "epoch 89: loss is 0.12275753915309906\n",
      "epoch 90: loss is 0.12264908105134964\n",
      "epoch 91: loss is 0.12254267185926437\n",
      "epoch 92: loss is 0.1224382221698761\n",
      "epoch 93: loss is 0.12233570218086243\n",
      "epoch 94: loss is 0.12223505228757858\n",
      "epoch 95: loss is 0.1221362054347992\n",
      "epoch 96: loss is 0.12203909456729889\n",
      "epoch 97: loss is 0.12194371968507767\n",
      "epoch 98: loss is 0.12184999138116837\n",
      "epoch 99: loss is 0.12175782769918442\n"
     ]
    }
   ],
   "source": [
    "mlp = torch.nn.Sequential(\n",
    "    torch.nn.Linear(2,100),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(100,3),\n",
    ")\n",
    "\n",
    "opt = torch.optim.SGD(mlp.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    opt.zero_grad() # zero out the gradients\n",
    "\n",
    "    z = mlp(X) # compute z values\n",
    "    loss = loss_fn(z,y) # compute loss\n",
    "\n",
    "    loss.backward() # compute gradients\n",
    "\n",
    "    opt.step() # apply gradients\n",
    "\n",
    "    print(f'epoch {epoch}: loss is {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear model accuracy: 0.9429429173469543\n",
      "mlp accuracy: 0.954954981803894\n"
     ]
    }
   ],
   "source": [
    "# Compute the accuracy of each model\n",
    "\n",
    "def accuracy(y_pred,y_true):\n",
    "    return (y_pred == y_true).float().mean()\n",
    "\n",
    "y_pred = linear_model(X).argmax(dim=1)\n",
    "acc = accuracy(y_pred,y)\n",
    "print(f'linear model accuracy: {acc.item()}')\n",
    "\n",
    "y_pred = mlp(X).argmax(dim=1)\n",
    "acc = accuracy(y_pred,y)\n",
    "print(f'mlp accuracy: {acc.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP has outperformed the linear model with it having an accuracy of 0.952 while the linear model accuracy is 0.949."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
