{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 3.1: Batching and Regularization\n",
    "\n",
    "In this lab you will learn how to set up a dataset to be processed in batches, rather than processing the entire dataset in each training iteration, and explore neural network regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'uci_id': 2, 'name': 'Adult', 'repository_url': 'https://archive.ics.uci.edu/dataset/2/adult', 'data_url': 'https://archive.ics.uci.edu/static/public/2/data.csv', 'abstract': 'Predict whether annual income of an individual exceeds $50K/yr based on census data. Also known as \"Census Income\" dataset. ', 'area': 'Social Science', 'tasks': ['Classification'], 'characteristics': ['Multivariate'], 'num_instances': 48842, 'num_features': 14, 'feature_types': ['Categorical', 'Integer'], 'demographics': ['Age', 'Income', 'Education Level', 'Other', 'Race', 'Sex'], 'target_col': ['income'], 'index_col': None, 'has_missing_values': 'yes', 'missing_values_symbol': 'NaN', 'year_of_dataset_creation': 1996, 'last_updated': 'Tue Sep 24 2024', 'dataset_doi': '10.24432/C5XW20', 'creators': ['Barry Becker', 'Ronny Kohavi'], 'intro_paper': None, 'additional_info': {'summary': \"Extraction was done by Barry Becker from the 1994 Census database.  A set of reasonably clean records was extracted using the following conditions: ((AAGE>16) && (AGI>100) && (AFNLWGT>1)&& (HRSWK>0))\\n\\nPrediction task is to determine whether a person's income is over $50,000 a year.\\n\", 'purpose': None, 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': 'Listing of attributes:\\r\\n\\r\\n>50K, <=50K.\\r\\n\\r\\nage: continuous.\\r\\nworkclass: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.\\r\\nfnlwgt: continuous.\\r\\neducation: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.\\r\\neducation-num: continuous.\\r\\nmarital-status: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.\\r\\noccupation: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.\\r\\nrelationship: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.\\r\\nrace: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.\\r\\nsex: Female, Male.\\r\\ncapital-gain: continuous.\\r\\ncapital-loss: continuous.\\r\\nhours-per-week: continuous.\\r\\nnative-country: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands.', 'citation': None}}\n",
      "              name     role         type      demographic  \\\n",
      "0              age  Feature      Integer              Age   \n",
      "1        workclass  Feature  Categorical           Income   \n",
      "2           fnlwgt  Feature      Integer             None   \n",
      "3        education  Feature  Categorical  Education Level   \n",
      "4    education-num  Feature      Integer  Education Level   \n",
      "5   marital-status  Feature  Categorical            Other   \n",
      "6       occupation  Feature  Categorical            Other   \n",
      "7     relationship  Feature  Categorical            Other   \n",
      "8             race  Feature  Categorical             Race   \n",
      "9              sex  Feature       Binary              Sex   \n",
      "10    capital-gain  Feature      Integer             None   \n",
      "11    capital-loss  Feature      Integer             None   \n",
      "12  hours-per-week  Feature      Integer             None   \n",
      "13  native-country  Feature  Categorical            Other   \n",
      "14          income   Target       Binary           Income   \n",
      "\n",
      "                                          description units missing_values  \n",
      "0                                                 N/A  None             no  \n",
      "1   Private, Self-emp-not-inc, Self-emp-inc, Feder...  None            yes  \n",
      "2                                                None  None             no  \n",
      "3    Bachelors, Some-college, 11th, HS-grad, Prof-...  None             no  \n",
      "4                                                None  None             no  \n",
      "5   Married-civ-spouse, Divorced, Never-married, S...  None             no  \n",
      "6   Tech-support, Craft-repair, Other-service, Sal...  None            yes  \n",
      "7   Wife, Own-child, Husband, Not-in-family, Other...  None             no  \n",
      "8   White, Asian-Pac-Islander, Amer-Indian-Eskimo,...  None             no  \n",
      "9                                       Female, Male.  None             no  \n",
      "10                                               None  None             no  \n",
      "11                                               None  None             no  \n",
      "12                                               None  None             no  \n",
      "13  United-States, Cambodia, England, Puerto-Rico,...  None            yes  \n",
      "14                                       >50K, <=50K.  None             no  \n"
     ]
    }
   ],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "adult = fetch_ucirepo(id=2) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = adult.data.features \n",
    "y = adult.data.targets \n",
    "  \n",
    "# metadata \n",
    "print(adult.metadata) \n",
    "  \n",
    "# variable information \n",
    "print(adult.variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
       "       'marital-status', 'occupation', 'relationship', 'race', 'sex',\n",
       "       'capital-gain', 'capital-loss', 'hours-per-week', 'native-country'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y['income'].map({'<=50K':0,'<=50K.':0,'>50K':1,'>50K.':1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I remove the missing values from the features and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad = X.isna().any(axis=1)\n",
    "X = X[~bad]\n",
    "y = y[~bad]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting only the numeric variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[['age','fnlwgt','education-num','capital-gain','capital-loss','hours-per-week']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.values\n",
    "X = X.values.astype('float64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the learning algorithm work more smoothly, we we will subtract the mean of each feature.\n",
    "\n",
    "Here `np.mean` calculates a mean, and `axis=0` tells NumPy to calculate the mean over the rows (calculate the mean of each column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X -= np.mean(X,axis=0)\n",
    "X /= np.std(X,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will convert our `X` and `y` arrays to torch Tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(X).float()\n",
    "y = torch.tensor(y).long()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "1. Divide the data into train and test splits.\n",
    "2. Create a neural network for this dataset.\n",
    "3. Use `TensorDataset` and `DataLoader` to batch the dataset during training.  \n",
    "4. Use `weight_decay` parameter to `optim.SGD` to introduce L2 regularization during training. Evaluate the effect of regularization on test set accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss 0.4376978278160095\n",
      "Epoch 1, loss 0.24951942265033722\n",
      "Epoch 2, loss 0.4959902763366699\n",
      "Epoch 3, loss 0.3605423867702484\n",
      "Epoch 4, loss 0.4009922444820404\n",
      "Epoch 5, loss 0.5468993186950684\n",
      "Epoch 6, loss 0.6846466660499573\n",
      "Epoch 7, loss 0.4402170181274414\n",
      "Epoch 8, loss 0.39482343196868896\n",
      "Epoch 9, loss 0.37771379947662354\n",
      "Epoch 10, loss 0.21278099715709686\n",
      "Epoch 11, loss 0.30503034591674805\n",
      "Epoch 12, loss 0.3026094436645508\n",
      "Epoch 13, loss 0.2786139249801636\n",
      "Epoch 14, loss 0.4710782468318939\n",
      "Epoch 15, loss 0.46451061964035034\n",
      "Epoch 16, loss 0.5290101766586304\n",
      "Epoch 17, loss 0.3755161464214325\n",
      "Epoch 18, loss 0.31772488355636597\n",
      "Epoch 19, loss 0.48607248067855835\n",
      "Epoch 20, loss 0.3249191641807556\n",
      "Epoch 21, loss 0.38933688402175903\n",
      "Epoch 22, loss 0.36900535225868225\n",
      "Epoch 23, loss 0.3623456656932831\n",
      "Epoch 24, loss 0.4290372133255005\n",
      "Epoch 25, loss 0.3215467631816864\n",
      "Epoch 26, loss 0.47005996108055115\n",
      "Epoch 27, loss 0.5331555008888245\n",
      "Epoch 28, loss 0.2603340446949005\n",
      "Epoch 29, loss 0.45886534452438354\n",
      "Epoch 30, loss 0.3398805558681488\n",
      "Epoch 31, loss 0.33240175247192383\n",
      "Epoch 32, loss 0.22328779101371765\n",
      "Epoch 33, loss 0.5431573987007141\n",
      "Epoch 34, loss 0.34762924909591675\n",
      "Epoch 35, loss 0.4111142158508301\n",
      "Epoch 36, loss 0.2520825266838074\n",
      "Epoch 37, loss 0.316532164812088\n",
      "Epoch 38, loss 0.28987008333206177\n",
      "Epoch 39, loss 0.5350692272186279\n",
      "Epoch 40, loss 0.3240247964859009\n",
      "Epoch 41, loss 0.346171498298645\n",
      "Epoch 42, loss 0.677726149559021\n",
      "Epoch 43, loss 0.13279059529304504\n",
      "Epoch 44, loss 0.3431183993816376\n",
      "Epoch 45, loss 0.41965317726135254\n",
      "Epoch 46, loss 0.32037538290023804\n",
      "Epoch 47, loss 0.6329307556152344\n",
      "Epoch 48, loss 0.33020126819610596\n",
      "Epoch 49, loss 0.3819095194339752\n",
      "Epoch 50, loss 0.46073734760284424\n",
      "Epoch 51, loss 0.302001416683197\n",
      "Epoch 52, loss 0.5830949544906616\n",
      "Epoch 53, loss 0.35974475741386414\n",
      "Epoch 54, loss 0.45473575592041016\n",
      "Epoch 55, loss 0.47705554962158203\n",
      "Epoch 56, loss 0.368024080991745\n",
      "Epoch 57, loss 0.1778062880039215\n",
      "Epoch 58, loss 0.3789816200733185\n",
      "Epoch 59, loss 0.4102090299129486\n",
      "Epoch 60, loss 0.5300571918487549\n",
      "Epoch 61, loss 0.25361284613609314\n",
      "Epoch 62, loss 0.37926721572875977\n",
      "Epoch 63, loss 0.3981536328792572\n",
      "Epoch 64, loss 0.22215092182159424\n",
      "Epoch 65, loss 0.34123891592025757\n",
      "Epoch 66, loss 0.5830374956130981\n",
      "Epoch 67, loss 0.2707347273826599\n",
      "Epoch 68, loss 0.6771588921546936\n",
      "Epoch 69, loss 0.34266823530197144\n",
      "Epoch 70, loss 0.4480808973312378\n",
      "Epoch 71, loss 0.4963356852531433\n",
      "Epoch 72, loss 0.3226059675216675\n",
      "Epoch 73, loss 0.35853761434555054\n",
      "Epoch 74, loss 0.6904300451278687\n",
      "Epoch 75, loss 0.4130728244781494\n",
      "Epoch 76, loss 0.2936755418777466\n",
      "Epoch 77, loss 0.4046236574649811\n",
      "Epoch 78, loss 0.2534148395061493\n",
      "Epoch 79, loss 0.17065681517124176\n",
      "Epoch 80, loss 0.2865121364593506\n",
      "Epoch 81, loss 0.6101980209350586\n",
      "Epoch 82, loss 0.5141997337341309\n",
      "Epoch 83, loss 0.41847726702690125\n",
      "Epoch 84, loss 0.43438446521759033\n",
      "Epoch 85, loss 0.5175608396530151\n",
      "Epoch 86, loss 0.3023552894592285\n",
      "Epoch 87, loss 0.2784176170825958\n",
      "Epoch 88, loss 0.31239986419677734\n",
      "Epoch 89, loss 0.2277803272008896\n",
      "Epoch 90, loss 0.20638886094093323\n",
      "Epoch 91, loss 0.3904450535774231\n",
      "Epoch 92, loss 0.285020649433136\n",
      "Epoch 93, loss 0.26532697677612305\n",
      "Epoch 94, loss 0.17123675346374512\n",
      "Epoch 95, loss 0.08252528309822083\n",
      "Epoch 96, loss 0.4827987849712372\n",
      "Epoch 97, loss 0.2551703453063965\n",
      "Epoch 98, loss 0.5602370500564575\n",
      "Epoch 99, loss 0.2977784276008606\n"
     ]
    }
   ],
   "source": [
    "# divide data into training and testing set\n",
    "n = X.shape[0]\n",
    "n_train = int(n*0.8)\n",
    "n_test = n - n_train\n",
    "\n",
    "X_train = X[:n_train]\n",
    "y_train = y[:n_train]\n",
    "X_test = X[n_train:]\n",
    "y_test = y[n_train:]\n",
    "\n",
    "# create neural network for this dataset\n",
    "from torch import nn\n",
    "\n",
    "nn_model = nn.Sequential(\n",
    "    nn.Linear(6, 100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, 100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, 2)\n",
    ")\n",
    "\n",
    "# use TensorDataset and DataLoader to batch dataset during training\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_data = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n",
    "\n",
    "# use weight_decay parameter to optim.SGD to introduce L2 regularization during training\n",
    "from torch import optim\n",
    "\n",
    "optimizer = optim.SGD(nn_model.parameters(), lr=0.01, weight_decay=0.01)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def train(model, train_loader, optimizer, loss_fn, n_epochs=100):\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_batch)\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f'Epoch {epoch}, loss {loss.item()}')\n",
    "\n",
    "train(nn_model, train_loader, optimizer, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8137532808398951\n"
     ]
    }
   ],
   "source": [
    "# evaluate the effect of regularization on test set accuracy\n",
    "def test(model, test_loader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            y_pred = model(X_batch)\n",
    "            _, predicted = torch.max(y_pred, 1)\n",
    "            total += y_batch.size(0)\n",
    "            correct += (predicted == y_batch).sum().item()\n",
    "    return correct / total\n",
    "\n",
    "print(test(nn_model, test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Achieved an accuracy of 81.38% on the test set with regularization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
