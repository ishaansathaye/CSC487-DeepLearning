{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 8.1 Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This week we will work up to creating an RNN text generator.  In today's lab you will explore different methods of text tokenization.   Here's an overview of what you will try to do.\n",
    "\n",
    "Imagine that our entire dataset consists of the following text:\n",
    "\n",
    "    hello world hello a b c\n",
    "\n",
    "We would first build a vocabulary of the words in the dataset:\n",
    "\n",
    "    0: hello\n",
    "    1: world\n",
    "    2: a\n",
    "    3: b\n",
    "    4: c\n",
    "\n",
    "Thus the dataset can be mapped to token indices:\n",
    "\n",
    "    0 1 0 2 3 4\n",
    "\n",
    "Now suppose that we have defined the maximum sequence length (`seq_len`) to be 3.  We will use each possible sequence as the input to our RNN, and the next token as the target.  Here are the possible input sequences and targets:\n",
    "\n",
    "    0 1 0 -> 2\n",
    "    1 0 2 -> 3\n",
    "    0 2 3 -> 4\n",
    "\n",
    "You will build a subclass of `Dataset` to find all possible sequences for a given dataset, either at the word or character level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will download the text of Shakespeare's sonnets and read it in as one long string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-02-25 20:46:43--  https://www.dropbox.com/scl/fi/7r68l64ijemidyb9lf80q/sonnets.txt?rlkey=udb47coatr2zbrk31hsfbr22y&dl=1\n",
      "Resolving www.dropbox.com (www.dropbox.com)... 2620:100:601b:18::a27d:812, 162.125.8.18\n",
      "Connecting to www.dropbox.com (www.dropbox.com)|2620:100:601b:18::a27d:812|:443... connected.\n",
      "ERROR: cannot verify www.dropbox.com's certificate, issued by ‘CN=DigiCert TLS RSA SHA256 2020 CA1,O=DigiCert Inc,C=US’:\n",
      "  Unable to locally verify the issuer's authority.\n",
      "To connect to www.dropbox.com insecurely, use `--no-check-certificate'.\n"
     ]
    }
   ],
   "source": [
    "!wget --no-clobber \"https://www.dropbox.com/scl/fi/7r68l64ijemidyb9lf80q/sonnets.txt?rlkey=udb47coatr2zbrk31hsfbr22y&dl=1\" -O sonnets.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = (open(\"sonnets.txt\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i\n",
      "\n",
      " from fairest creatures we desire increase,\n",
      " that thereby beauty's rose might never die,\n",
      " but as the riper should by time decease,\n",
      " his tender heir might bear his memory:\n",
      " but thou, contracted to thine own bright eyes,\n",
      " feed'st thy light's flame with self-substantial fuel,\n",
      " making a famine where abundance lies,\n",
      " thy self thy foe, to thy sweet self too cruel:\n",
      " thou that art now the world's fresh ornament,\n",
      " and only herald to the gaudy spring,\n",
      " within thine own bud buriest thy content,\n",
      " and tender churl mak'st waste in niggarding:\n",
      "   pity the world, or else this glutton be,\n",
      "   to eat the world's due, by the grave and thee.\n",
      "\n",
      " ii\n",
      "\n",
      " when forty winters shall besiege thy brow,\n",
      " and dig deep trenches in thy beauty's field,\n",
      " thy youth's proud livery so gazed on now,\n",
      " will be a tatter'd weed of small worth held:\n",
      " then being asked, where all thy beauty lies,\n",
      " where all the treasure of thy lusty days;\n",
      " to say, within thine own deep sunken eyes,\n",
      " were an all-eating shame, and thriftless praise.\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "1. Prepare a vocabulary of the unique words in the dataset.  (For simplicity's sake you can leave the punctuation in.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 4901\n",
      "Sample words: [\"''tis\", \"'Amen'\", \"'Fair,\", \"'Gainst\", \"'Had\", \"'I\", \"'No'\", \"'Now\", \"'Since\", \"'This\", \"'Thou\", \"'Thus\", \"'Thy\", \"'Tis\", \"'Truth\", \"'Will'\", \"'Will',\", \"'Will,'\", \"'Will.'\", \"'fore\"]\n"
     ]
    }
   ],
   "source": [
    "# Read the sonnets.txt file\n",
    "with open(\"sonnets.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Split text into words (punctuation remains attached)\n",
    "words = text.split()\n",
    "\n",
    "# Compute the unique words and sort them (optional)\n",
    "vocab = sorted(set(words))\n",
    "\n",
    "# Print out the vocabulary size and a few sample words\n",
    "print(\"Vocabulary size:\", len(vocab))\n",
    "print(\"Sample words:\", vocab[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Now you will make a Dataset subclass that can return sequences of tokens, encoded as integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordDataset(Dataset):\n",
    "  def __init__(self,text,seq_len=100):\n",
    "    self.seq_len = seq_len\n",
    "    # add code to compute the vocabulary (copied from exercise 1)\n",
    "    # add code to convert the text to a sequence of word indices\n",
    "    self.seq_len = seq_len\n",
    "    self.vocab = sorted(set(text.split()))\n",
    "    self.word_to_idx = {word: idx for idx, word in enumerate(self.vocab)}\n",
    "    self.idx_to_word = {idx: word for idx, word in enumerate(self.vocab)}\n",
    "    self.data = [self.word_to_idx[word] for word in text.split()]\n",
    "\n",
    "  def __len__(self):\n",
    "    # replace this with code to return the number of possible sub-sequences\n",
    "    return len(self.data) - self.seq_len\n",
    "\n",
    "  def __getitem__(self,i):\n",
    "    # replace this with code to return a sequence of length seq_len of token indices starting at i, and the index of token i+seq_len as the label\n",
    "    return (self.data[i:i+self.seq_len], self.data[i+self.seq_len])\n",
    "\n",
    "  def decode(self,tokens):\n",
    "    # replace this with code to convert a sequence of tokens back into a string\n",
    "    return ' '.join(self.idx_to_word[token] for token in tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Verify that your class can successfully encode and decode sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded sequence (indices): [4, 3, 1, 6, 7]\n",
      "Label index: 0\n",
      "Decoded sequence: shall i compare thee to\n",
      "Decoded label: a\n",
      "Decoded text: shall i compare thee to a summer's day\n"
     ]
    }
   ],
   "source": [
    "# sentence from sonnets.txt\n",
    "text = \"shall i compare thee to a summer's day\"\n",
    "seq_len = 5\n",
    "dataset = WordDataset(text, seq_len=seq_len)\n",
    "seq, label = dataset[0]\n",
    "\n",
    "\n",
    "print(\"Encoded sequence (indices):\", seq)\n",
    "print(\"Label index:\", label)\n",
    "\n",
    "\n",
    "decoded_seq = dataset.decode(seq)\n",
    "print(\"Decoded sequence:\", decoded_seq)\n",
    "\n",
    "\n",
    "decoded_label = dataset.idx_to_word[label]\n",
    "print(\"Decoded label:\", decoded_label)\n",
    "\n",
    "decoded_text = dataset.decode(dataset.data)\n",
    "print(\"Decoded text:\", decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Do the exercise again, but this time at the character level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharacterDataset(Dataset):\n",
    "  def __init__(self,text,seq_len=100):\n",
    "    self.seq_len = seq_len\n",
    "    # add code to compute the vocabulary of unique characters\n",
    "    # add code to convert the text to a sequence of character indices\n",
    "    self.seq_len = seq_len\n",
    "    self.vocab = sorted(set(text))\n",
    "    self.char_to_idx = {char: idx for idx, char in enumerate(self.vocab)}\n",
    "    self.idx_to_char = {idx: char for idx, char in enumerate(self.vocab)}\n",
    "    self.data = [self.char_to_idx[c] for c in text]\n",
    "\n",
    "  def __len__(self):\n",
    "    # replace this with code to return the number of possible sub-sequences\n",
    "    return len(self.data) - self.seq_len\n",
    "\n",
    "  def __getitem__(self,i):\n",
    "    # replace this with code to return the sequence of length seq_len of token indices starting at i, and the index of token i+seq_len as the label\n",
    "    return (self.data[i:i+self.seq_len], self.data[i+self.seq_len])\n",
    "\n",
    "  def decode(self,tokens):\n",
    "    # replace this with code to convert a sequence of tokens back into a string\n",
    "    return ''.join(self.idx_to_char[token] for token in tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Compare the number of sequences for each tokenization method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word-level number of sequences: 5\n",
      "Character-level number of sequences: 28\n"
     ]
    }
   ],
   "source": [
    "text = \"shall i compare thee to a summer's day\"\n",
    "seq_len_word = 3\n",
    "seq_len_char = 10\n",
    "\n",
    "word_dataset = WordDataset(text, seq_len=seq_len_word)\n",
    "num_word_sequences = len(word_dataset)\n",
    "\n",
    "char_dataset = CharacterDataset(text, seq_len=seq_len_char)\n",
    "num_char_sequences = len(char_dataset)\n",
    "\n",
    "print(\"Word-level number of sequences:\", num_word_sequences)\n",
    "print(\"Character-level number of sequences:\", num_char_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Optional: implement the byte pair encoding algorithm to make a Dataset class that uses word parts."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
