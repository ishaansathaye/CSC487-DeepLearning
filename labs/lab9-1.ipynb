{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8_JRrHbuqxD"
      },
      "source": [
        "### Lab 9.1 Attention Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wca8laewuqxF"
      },
      "source": [
        "This week you will experimenet with attention-based models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "id": "cQeu1KuyuqxF",
        "outputId": "cba59da9-aff1-45da-c968-fdd9ac36b6a0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9kaAydLuqxG"
      },
      "source": [
        "1. Complete the following implementation of scaled dot-product attention.   Run the code cell to verify that the output shape is what it should be.\n",
        "\n",
        "*Note: you can use `scores = scores.masked_fill(...)` to fill in values where the mask is True.  Fill in -1e9 as the score for masked values.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_RxPBjrsuqxG"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 5, 8])"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def attention(Q, K, V, mask=None):\n",
        "    \"\"\"\n",
        "    Computes scaled dot-product attention.\n",
        "\n",
        "    Compute scores as Q*K^T. \n",
        "    Optionally mask out score values to -1e9 where the mask is True.\n",
        "    Divide by sqrt(d_k).\n",
        "    Compute softmax on scores along the rows to obtain attention weights.\n",
        "    Matrix multiply attention weights by values.\n",
        "\n",
        "    Arguments:\n",
        "      Q: queries [B,L,d_k]\n",
        "      K: keys    [B,S,d_k]\n",
        "      V: values  [B,S,d_v]\n",
        "      mask: optional Boolean mask where True means hidden [B,L,S]\n",
        "\n",
        "    Returns:\n",
        "      Sequence of context vectors of shape [B,L,d_v]\n",
        "    \"\"\"\n",
        "    scores = torch.matmul(Q, K.transpose(-2, -1))\n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(mask, -1e9)\n",
        "    \n",
        "    scores = scores / np.sqrt(Q.size(-1))\n",
        "    weights = torch.softmax(scores, dim=-1)\n",
        "    out = torch.matmul(weights, V)\n",
        "    return out\n",
        "\n",
        "\n",
        "Q = torch.rand(1, 5, 64)\n",
        "K = torch.rand(1, 10, 64)\n",
        "V = torch.rand(1, 10, 8)\n",
        "mask = (torch.rand(1, 5, 10) > 0.5)\n",
        "\n",
        "y = attention(Q, K, V, mask=mask)\n",
        "\n",
        "y.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following code creates classes to build a Transformer-style decoder for generating sequences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "j4YQt8IeuqxH"
      },
      "outputs": [],
      "source": [
        "class AttentionHead(nn.Module):\n",
        "    def __init__(self,d_model,d_k):\n",
        "        super().__init__()\n",
        "        self.WQ = nn.Linear(d_model,d_k)\n",
        "        self.WK = nn.Linear(d_model,d_k)\n",
        "        self.WV = nn.Linear(d_model,d_k)\n",
        "\n",
        "    def forward(self,Q,K,V,mask=None):\n",
        "        \"\"\" Compute attention head.\n",
        "\n",
        "            Project the input to queries, keys, and values, and then apply attention.\n",
        "            Arguments:\n",
        "                Q: queries [B,L,d_model]\n",
        "                K: keys    [B,S,d_model]\n",
        "                V: values  [B,L,d_model]\n",
        "                mask: optional Boolean mask where True means hidden [B,L,S]\n",
        "            Output:\n",
        "                Context vectors [B,L,d_k]\n",
        "        \"\"\"\n",
        "        # apply linear projections to queries, keys, and values followed by masked attention\n",
        "        return attention(self.WQ(Q),self.WK(K),self.WV(V),mask=mask)\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self,d_model=512,num_heads=8):\n",
        "        super().__init__()\n",
        "        self.heads = []\n",
        "        d_k = d_model // num_heads\n",
        "        self.heads = nn.ModuleList([AttentionHead(d_model,d_k) for head in range(num_heads)])\n",
        "        self.W = nn.Linear(d_model,d_model)\n",
        "\n",
        "    def forward(self,Q,K,V,mask=None):\n",
        "        \"\"\" Compute multi-head attention.\n",
        "\n",
        "            Applies attention num_heads times, concatenates the results, and applies a final linear projection.\n",
        "            Arguments:\n",
        "                Q: queries [B,L,d_model]\n",
        "                K: keys    [B,S,d_model]\n",
        "                V: values  [B,L,d_model]\n",
        "                mask: optional Boolean mask where True means hidden [B,L,S]\n",
        "            Output:\n",
        "               result of multi-head attention [B,L,d_model]\n",
        "        \"\"\"\n",
        "        # compute each attention head and concatenate\n",
        "        h = torch.cat([head(Q,K,V,mask=mask) for head in self.heads],dim=-1)\n",
        "\n",
        "        # apply output projection\n",
        "        return self.W(h)\n",
        "\n",
        "class SelfAttentionBlock(nn.Module):\n",
        "    def __init__(self,d_model=512,num_heads=8,d_ff=2048):\n",
        "        super().__init__()\n",
        "        self.multi_head_attention = MultiHeadAttention(d_model,num_heads)\n",
        "        self.ln1 = nn.LayerNorm(d_model)\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(d_model,d_ff),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_ff,d_model),\n",
        "        )\n",
        "        self.ln2 = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self,x,mask=None):\n",
        "        \"\"\" Compute self attention block.\n",
        "\n",
        "            Arguments:\n",
        "                x: input sequence [B,S,d_model]\n",
        "                mask: optional Boolean mask where True means hidden [B,L,S]\n",
        "            Output:\n",
        "               result of attention block [B,L,d_model]\n",
        "        \"\"\"\n",
        "        # compute multi-head attention\n",
        "        mha = self.multi_head_attention(x,x,x,mask=mask)\n",
        "\n",
        "        # residual connection and layer normalization\n",
        "        x = self.ln1(mha + x)\n",
        "\n",
        "        # compute feed-forward network\n",
        "        ff = self.feed_forward(x)\n",
        "\n",
        "        # residual connection and layer normalization\n",
        "        x = self.ln2(ff + x)\n",
        "\n",
        "        return x\n",
        "    \n",
        "class PositionalEmbedding(nn.Module):\n",
        "    def __init__(self,max_seq_len,d_model):\n",
        "        super().__init__()\n",
        "        self.positional_embedding = nn.Embedding(max_seq_len,d_model)\n",
        "\n",
        "    def forward(self,x):\n",
        "        \"\"\" Adds a positional embedding.\n",
        "\n",
        "            Arguments:\n",
        "                x: input token sequence [B,S,d_model]\n",
        "            Output:\n",
        "               sequence with positional embedding added [B,S,d_model]\n",
        "        \"\"\"\n",
        "        # get sequence length\n",
        "        N = x.shape[1]\n",
        "\n",
        "        # look up positional embedding vectors\n",
        "        pe = self.positional_embedding(torch.arange(N).to(x.device)) # [N,d_model]\n",
        "\n",
        "        # add to input\n",
        "        x = x + pe[None,...] # [B,N,d_model]\n",
        "        \n",
        "        return x\n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self,vocabulary_size,max_seq_len,\n",
        "                      d_model=512,num_heads=8,d_ff=2048,num_blocks=6):\n",
        "        super().__init__()\n",
        "        self.blocks = nn.ModuleList([SelfAttentionBlock(d_model,num_heads,d_ff) for b in range(num_blocks)])\n",
        "        self.token_embedding = nn.Embedding(vocabulary_size,d_model)\n",
        "        self.output = nn.Linear(d_model,vocabulary_size)\n",
        "        self.positional_embedding = PositionalEmbedding(max_seq_len,d_model)\n",
        "\n",
        "    def forward(self,x,mask=None):\n",
        "        \"\"\" Computes the decoded sequence:\n",
        "\n",
        "            Convert input to token embedding vectors\n",
        "            Add positional embedding to input\n",
        "            Apply self-attention blocks with mask\n",
        "            Compute output\n",
        "\n",
        "            Arguments:\n",
        "                x: input token sequence [B,S]\n",
        "                mask: optional Boolean mask where false means hidden [B,S]\n",
        "            Output:\n",
        "               sequence predictions [B,S,output_size]\n",
        "        \"\"\"\n",
        "        # look up embedding vectors for tokens\n",
        "        x = self.token_embedding(x) # [B,S,d_model]\n",
        "\n",
        "        # apply positional embedding\n",
        "        x = self.positional_embedding(x) # [B,S,d_model]\n",
        "        \n",
        "        # apply sequence of masked self-attention blocks\n",
        "        for block in self.blocks:\n",
        "            x = block(x,mask=mask) # [B,S,d_model]\n",
        "        \n",
        "        # produce sequence of output vectors\n",
        "        y = self.output(x) # [B,S,vocabulary_size]\n",
        "\n",
        "        return y\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This function produces masks appropriate for sequence prediction.  The mask ensures that the output token at time t+1 only sees the generated sequence up to time t."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[False,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
              "         [False, False,  True,  True,  True,  True,  True,  True,  True,  True],\n",
              "         [False, False, False,  True,  True,  True,  True,  True,  True,  True],\n",
              "         [False, False, False, False,  True,  True,  True,  True,  True,  True],\n",
              "         [False, False, False, False, False,  True,  True,  True,  True,  True],\n",
              "         [False, False, False, False, False, False,  True,  True,  True,  True],\n",
              "         [False, False, False, False, False, False, False,  True,  True,  True],\n",
              "         [False, False, False, False, False, False, False, False,  True,  True],\n",
              "         [False, False, False, False, False, False, False, False, False,  True],\n",
              "         [False, False, False, False, False, False, False, False, False, False]]])"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def make_mask(seq_len):\n",
        "    \"\"\" Make a mask for sequence prediction. \"\"\"\n",
        "    return (torch.triu(torch.ones((1,seq_len,seq_len)), diagonal=1)==1)\n",
        "\n",
        "make_mask(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we will make a sequence of integers and see if the Transformer decoder can learn the sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
              "          18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
              "          36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
              "          54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71,\n",
              "          72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89,\n",
              "          90, 91, 92, 93, 94, 95, 96, 97, 98]]),\n",
              " tensor([[ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
              "          19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36,\n",
              "          37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54,\n",
              "          55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72,\n",
              "          73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90,\n",
              "          91, 92, 93, 94, 95, 96, 97, 98, 99]]))"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "seq = torch.arange(100)\n",
        "x = seq[:-1][None,...]\n",
        "y = seq[1:][None,...]\n",
        "mask = make_mask(x.shape[1])\n",
        "x,y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 4.686927795410156\n",
            "1 2.9648141860961914\n",
            "2 2.588106393814087\n",
            "3 2.3566484451293945\n",
            "4 1.7879502773284912\n",
            "5 1.2888712882995605\n",
            "6 0.7589181661605835\n",
            "7 0.5663195252418518\n",
            "8 0.39252418279647827\n",
            "9 0.2695852220058441\n",
            "10 0.1874636709690094\n",
            "11 0.12777379155158997\n",
            "12 0.08888237923383713\n",
            "13 0.0635567381978035\n",
            "14 0.04658680781722069\n",
            "15 0.03535320609807968\n",
            "16 0.027867989614605904\n",
            "17 0.022476395592093468\n",
            "18 0.01819978654384613\n",
            "19 0.014668811112642288\n",
            "20 0.011812885291874409\n",
            "21 0.00959496758878231\n",
            "22 0.007916177622973919\n",
            "23 0.006650303490459919\n",
            "24 0.005682090763002634\n",
            "25 0.00492187449708581\n",
            "26 0.004310456104576588\n",
            "27 0.0038083535619080067\n",
            "28 0.0033909459598362446\n",
            "29 0.0030427398160099983\n",
            "30 0.0027510453946888447\n",
            "31 0.002506152493879199\n",
            "32 0.0023002822417765856\n",
            "33 0.0021264569368213415\n",
            "34 0.001978531712666154\n",
            "35 0.0018513250397518277\n",
            "36 0.0017400712240487337\n",
            "37 0.0016414851415902376\n",
            "38 0.0015528165968135\n",
            "39 0.0014722332125529647\n",
            "40 0.0013984597753733397\n",
            "41 0.0013306493638083339\n",
            "42 0.0012683414388448\n",
            "43 0.0012111724354326725\n",
            "44 0.0011589186033234\n",
            "45 0.0011113682994619012\n",
            "46 0.0010682231513783336\n",
            "47 0.0010291824582964182\n",
            "48 0.0009939060546457767\n",
            "49 0.0009620194905437529\n",
            "50 0.0009331220644526184\n",
            "51 0.0009069088846445084\n",
            "52 0.0008830586448311806\n",
            "53 0.0008612976525910199\n",
            "54 0.0008413008181378245\n",
            "55 0.0008229325176216662\n",
            "56 0.0008059800020419061\n",
            "57 0.000790178484749049\n",
            "58 0.0007755076512694359\n",
            "59 0.0007617664523422718\n",
            "60 0.0007488590781576931\n",
            "61 0.0007367876241914928\n",
            "62 0.0007253993535414338\n",
            "63 0.0007147054420784116\n",
            "64 0.0007045766687951982\n",
            "65 0.0006950014503672719\n",
            "66 0.000685915641952306\n",
            "67 0.0006773101049475372\n",
            "68 0.0006691110320389271\n",
            "69 0.0006612957804463804\n",
            "70 0.0006537992740049958\n",
            "71 0.0006466853083111346\n",
            "72 0.0006398470723070204\n",
            "73 0.0006332902703434229\n",
            "74 0.0006269968580454588\n",
            "75 0.0006209284183569252\n",
            "76 0.0006150621338747442\n",
            "77 0.0006094028358347714\n",
            "78 0.0006039348081685603\n",
            "79 0.0005986737669445574\n",
            "80 0.0005935727385804057\n",
            "81 0.0005886087892577052\n",
            "82 0.0005838326178491116\n",
            "83 0.0005791719886474311\n",
            "84 0.0005746519891545177\n",
            "85 0.000570205447729677\n",
            "86 0.0005658960435539484\n",
            "87 0.0005616937414743006\n",
            "88 0.0005575866089202464\n",
            "89 0.0005536032258532941\n",
            "90 0.0005496740341186523\n",
            "91 0.0005457942024804652\n",
            "92 0.0005420225788839161\n",
            "93 0.0005383291863836348\n",
            "94 0.0005347273545339704\n",
            "95 0.0005311603308655322\n",
            "96 0.0005277040181681514\n",
            "97 0.0005242513143457472\n",
            "98 0.0005208575748838484\n",
            "99 0.0005175468395464122\n"
          ]
        }
      ],
      "source": [
        "steps = 100\n",
        "\n",
        "model = TransformerDecoder(vocabulary_size=100,max_seq_len=x.shape[1],\n",
        "                           d_model=64,num_heads=8,d_ff=512,num_blocks=3\n",
        "                           )\n",
        "\n",
        "\n",
        "opt = torch.optim.Adam(model.parameters(),lr=.01)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "for step in range(steps):\n",
        "    model.train()\n",
        "    opt.zero_grad()\n",
        "\n",
        "    y_pred = model(x,mask)\n",
        "    loss = loss_fn(y_pred.view(-1,y_pred.shape[-1]),y.view(-1))\n",
        "    loss.backward()\n",
        "\n",
        "    opt.step()\n",
        "\n",
        "    print(step,loss.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If the Transformer has learned the sequence correctly, this output will read 1, 2, 3, ..., 97, 98, 99."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
              "         19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36,\n",
              "         37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54,\n",
              "         55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72,\n",
              "         73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90,\n",
              "         91, 92, 93, 94, 95, 96, 97, 98, 99]])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.argmax(model(x),-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. What size context does the Transformer need in order to learn the above sequence?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "99"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "max_seq_len = x.shape[1]\n",
        "max_seq_len"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The model need to be able to attend over the entire sequence of length 99 in order to learn the sequence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. Now design a pattern that requires a larger context and see if the Transformer can learn it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input sequences:\n",
            " tensor([[82, 73, 64, 46, 30, 25, 90, 43, 45, 73, 83, 52, 97,  9, 11, 95, 45, 50,\n",
            "         69,  5, 22, 83, 39,  9, 84,  2, 21, 70, 29, 97, 67, 38, 93, 22, 67, 59,\n",
            "         29,  5, 14, 64, 42, 82, 35, 37, 12, 75, 15, 95, 33, 61],\n",
            "        [59, 64, 78, 55,  4, 52, 63,  1,  6, 25, 68, 57, 57, 28, 65, 39, 48, 36,\n",
            "         69, 54, 15, 60, 75, 47, 22, 96, 42, 93, 22, 17, 50, 91, 18, 80, 80, 63,\n",
            "         14,  6, 64, 43, 99, 16, 76, 13, 83, 28, 94, 86, 13, 31],\n",
            "        [82, 26, 84,  1, 44, 70,  3, 35, 19,  8, 93, 58, 63,  9, 98, 68,  6, 14,\n",
            "         53, 89, 49, 53,  8, 22, 59,  3, 71, 45, 95, 57, 16, 89, 34, 19, 13, 14,\n",
            "          8, 99, 53, 67, 36,  1, 93, 72, 90, 76, 52, 48, 24, 18],\n",
            "        [33, 61,  0, 94, 97, 68,  2, 56, 26, 41, 92, 51,  6, 24, 72, 37, 43, 62,\n",
            "         71, 68, 78, 59, 89, 44, 51, 53, 45, 41, 35, 28,  3, 16, 69, 97, 30, 89,\n",
            "          3, 65, 88, 33, 79, 22, 76, 87, 57, 26, 91, 69, 71, 49],\n",
            "        [97, 44, 93, 89,  9, 93, 12,  2, 11, 98, 66, 75, 66, 14, 76, 78, 70, 72,\n",
            "         62, 99, 39, 95, 21, 92, 83, 66, 28, 59, 81, 19, 68, 31,  7, 75, 93, 90,\n",
            "          9, 49, 39, 22, 42,  9,  1, 44, 95, 22, 83, 31, 11, 24],\n",
            "        [46, 78, 21, 80, 50, 73, 74, 16,  7, 10, 11, 31, 93, 61, 96, 87, 78,  4,\n",
            "         24, 50, 16, 71, 70, 60, 28, 47, 12, 82,  1, 75, 10, 84, 35, 19, 92, 67,\n",
            "          4, 46, 99, 93, 60,  9,  7, 87, 51,  7, 66, 40,  5, 66],\n",
            "        [19,  6, 97, 38, 29, 36, 79,  6, 53,  7, 48, 92, 44, 22,  7, 77, 28, 26,\n",
            "         87, 64, 91, 89, 11, 60, 47, 87, 96, 39, 63, 30, 53, 78, 51, 79, 63, 31,\n",
            "         50,  6, 88, 29, 38, 52, 15, 42, 38, 60, 34,  0,  5, 81],\n",
            "        [61, 84, 94, 63,  8,  0, 29, 92, 95, 15, 77, 19, 70, 49, 78, 72, 96, 37,\n",
            "         57,  2, 51, 64, 83, 54, 97, 59, 19, 28, 27, 18, 40, 46, 21, 93, 86, 67,\n",
            "          0, 44, 51, 51, 65, 69, 50, 67, 23, 20, 45, 24, 48, 17],\n",
            "        [ 5, 20, 97, 84, 98, 73, 29, 54, 30,  2, 17, 72,  3, 62,  4,  7, 48, 39,\n",
            "         36, 52, 90, 86, 27, 66, 10,  2, 44, 72,  7, 20, 16, 27, 17, 73, 94, 21,\n",
            "         40, 46, 37, 97, 56, 19, 99, 73,  9, 12, 12, 77, 71, 99],\n",
            "        [80, 67, 91, 97, 42, 36, 34, 49, 72, 72, 85,  4, 37, 22, 88, 61, 26, 82,\n",
            "         61, 58, 55, 81, 42, 96, 62, 70, 75, 71, 81, 41,  3, 91,  2, 97, 97, 24,\n",
            "         31, 66, 22, 73,  6, 77, 69,  0, 32, 59, 20, 74, 69, 12],\n",
            "        [27, 60, 41, 32, 30, 19, 80, 49, 27, 61, 75, 52, 92, 31,  5, 39, 75, 64,\n",
            "         76, 45,  9, 11, 11, 55, 56, 12, 31, 59, 48, 75, 26, 30, 87,  4, 35, 96,\n",
            "         71, 28,  9, 30, 74, 90, 24, 79, 29, 22, 16, 41, 88, 42],\n",
            "        [36, 52, 92, 96,  9, 85, 95, 30, 36, 51, 97, 67,  2, 84, 56, 26, 95, 83,\n",
            "         90,  6, 49, 43,  0, 61, 87, 86, 98, 62, 31,  7, 41, 95,  7, 16,  6, 31,\n",
            "         53, 28, 57, 12, 97, 92, 78, 10, 41, 93, 30, 35, 66, 82],\n",
            "        [14, 21, 36, 29, 61, 60, 13, 25, 79, 54, 54, 89, 60, 21, 70, 12, 36, 42,\n",
            "         67, 82, 70, 17,  3, 50, 90, 40, 26, 66, 96, 56,  3, 69, 55, 37, 63, 40,\n",
            "         58, 26, 39, 15,  7,  3, 35, 59,  3, 46, 36, 63, 56,  7],\n",
            "        [71, 29, 79, 22, 83, 76, 88, 22, 69, 57, 81, 38, 98, 20, 53, 68, 45, 80,\n",
            "          6, 29, 18, 69, 36, 68, 42, 87, 64, 52, 91, 20, 96, 13, 25, 28, 91, 55,\n",
            "         26, 47, 83, 56, 61, 57, 15, 31,  5, 66, 94, 48, 32, 43],\n",
            "        [ 2, 98, 58, 54, 58, 31, 22, 76, 92, 53, 95, 46, 37, 40, 70,  9, 99, 23,\n",
            "         50, 57, 93, 97, 89, 31, 33,  6,  9, 37,  5, 77, 16, 41, 48,  7, 22,  4,\n",
            "         27, 74, 27, 60, 31, 14, 49, 72, 99, 95, 71, 30, 19, 82],\n",
            "        [96, 25, 68, 52, 50, 49, 45, 87, 67, 45, 84, 79, 73, 61, 41, 87, 76, 72,\n",
            "         49, 35, 95, 69,  9, 79,  3, 67, 10, 62, 59, 15, 58, 21,  0, 98, 57, 67,\n",
            "         16, 27, 69, 44, 23, 89, 56, 52, 12, 43, 74, 42, 86, 26]])\n",
            "Target (reversed) sequences:\n",
            " tensor([[61, 33, 95, 15, 75, 12, 37, 35, 82, 42, 64, 14,  5, 29, 59, 67, 22, 93,\n",
            "         38, 67, 97, 29, 70, 21,  2, 84,  9, 39, 83, 22,  5, 69, 50, 45, 95, 11,\n",
            "          9, 97, 52, 83, 73, 45, 43, 90, 25, 30, 46, 64, 73, 82],\n",
            "        [31, 13, 86, 94, 28, 83, 13, 76, 16, 99, 43, 64,  6, 14, 63, 80, 80, 18,\n",
            "         91, 50, 17, 22, 93, 42, 96, 22, 47, 75, 60, 15, 54, 69, 36, 48, 39, 65,\n",
            "         28, 57, 57, 68, 25,  6,  1, 63, 52,  4, 55, 78, 64, 59],\n",
            "        [18, 24, 48, 52, 76, 90, 72, 93,  1, 36, 67, 53, 99,  8, 14, 13, 19, 34,\n",
            "         89, 16, 57, 95, 45, 71,  3, 59, 22,  8, 53, 49, 89, 53, 14,  6, 68, 98,\n",
            "          9, 63, 58, 93,  8, 19, 35,  3, 70, 44,  1, 84, 26, 82],\n",
            "        [49, 71, 69, 91, 26, 57, 87, 76, 22, 79, 33, 88, 65,  3, 89, 30, 97, 69,\n",
            "         16,  3, 28, 35, 41, 45, 53, 51, 44, 89, 59, 78, 68, 71, 62, 43, 37, 72,\n",
            "         24,  6, 51, 92, 41, 26, 56,  2, 68, 97, 94,  0, 61, 33],\n",
            "        [24, 11, 31, 83, 22, 95, 44,  1,  9, 42, 22, 39, 49,  9, 90, 93, 75,  7,\n",
            "         31, 68, 19, 81, 59, 28, 66, 83, 92, 21, 95, 39, 99, 62, 72, 70, 78, 76,\n",
            "         14, 66, 75, 66, 98, 11,  2, 12, 93,  9, 89, 93, 44, 97],\n",
            "        [66,  5, 40, 66,  7, 51, 87,  7,  9, 60, 93, 99, 46,  4, 67, 92, 19, 35,\n",
            "         84, 10, 75,  1, 82, 12, 47, 28, 60, 70, 71, 16, 50, 24,  4, 78, 87, 96,\n",
            "         61, 93, 31, 11, 10,  7, 16, 74, 73, 50, 80, 21, 78, 46],\n",
            "        [81,  5,  0, 34, 60, 38, 42, 15, 52, 38, 29, 88,  6, 50, 31, 63, 79, 51,\n",
            "         78, 53, 30, 63, 39, 96, 87, 47, 60, 11, 89, 91, 64, 87, 26, 28, 77,  7,\n",
            "         22, 44, 92, 48,  7, 53,  6, 79, 36, 29, 38, 97,  6, 19],\n",
            "        [17, 48, 24, 45, 20, 23, 67, 50, 69, 65, 51, 51, 44,  0, 67, 86, 93, 21,\n",
            "         46, 40, 18, 27, 28, 19, 59, 97, 54, 83, 64, 51,  2, 57, 37, 96, 72, 78,\n",
            "         49, 70, 19, 77, 15, 95, 92, 29,  0,  8, 63, 94, 84, 61],\n",
            "        [99, 71, 77, 12, 12,  9, 73, 99, 19, 56, 97, 37, 46, 40, 21, 94, 73, 17,\n",
            "         27, 16, 20,  7, 72, 44,  2, 10, 66, 27, 86, 90, 52, 36, 39, 48,  7,  4,\n",
            "         62,  3, 72, 17,  2, 30, 54, 29, 73, 98, 84, 97, 20,  5],\n",
            "        [12, 69, 74, 20, 59, 32,  0, 69, 77,  6, 73, 22, 66, 31, 24, 97, 97,  2,\n",
            "         91,  3, 41, 81, 71, 75, 70, 62, 96, 42, 81, 55, 58, 61, 82, 26, 61, 88,\n",
            "         22, 37,  4, 85, 72, 72, 49, 34, 36, 42, 97, 91, 67, 80],\n",
            "        [42, 88, 41, 16, 22, 29, 79, 24, 90, 74, 30,  9, 28, 71, 96, 35,  4, 87,\n",
            "         30, 26, 75, 48, 59, 31, 12, 56, 55, 11, 11,  9, 45, 76, 64, 75, 39,  5,\n",
            "         31, 92, 52, 75, 61, 27, 49, 80, 19, 30, 32, 41, 60, 27],\n",
            "        [82, 66, 35, 30, 93, 41, 10, 78, 92, 97, 12, 57, 28, 53, 31,  6, 16,  7,\n",
            "         95, 41,  7, 31, 62, 98, 86, 87, 61,  0, 43, 49,  6, 90, 83, 95, 26, 56,\n",
            "         84,  2, 67, 97, 51, 36, 30, 95, 85,  9, 96, 92, 52, 36],\n",
            "        [ 7, 56, 63, 36, 46,  3, 59, 35,  3,  7, 15, 39, 26, 58, 40, 63, 37, 55,\n",
            "         69,  3, 56, 96, 66, 26, 40, 90, 50,  3, 17, 70, 82, 67, 42, 36, 12, 70,\n",
            "         21, 60, 89, 54, 54, 79, 25, 13, 60, 61, 29, 36, 21, 14],\n",
            "        [43, 32, 48, 94, 66,  5, 31, 15, 57, 61, 56, 83, 47, 26, 55, 91, 28, 25,\n",
            "         13, 96, 20, 91, 52, 64, 87, 42, 68, 36, 69, 18, 29,  6, 80, 45, 68, 53,\n",
            "         20, 98, 38, 81, 57, 69, 22, 88, 76, 83, 22, 79, 29, 71],\n",
            "        [82, 19, 30, 71, 95, 99, 72, 49, 14, 31, 60, 27, 74, 27,  4, 22,  7, 48,\n",
            "         41, 16, 77,  5, 37,  9,  6, 33, 31, 89, 97, 93, 57, 50, 23, 99,  9, 70,\n",
            "         40, 37, 46, 95, 53, 92, 76, 22, 31, 58, 54, 58, 98,  2],\n",
            "        [26, 86, 42, 74, 43, 12, 52, 56, 89, 23, 44, 69, 27, 16, 67, 57, 98,  0,\n",
            "         21, 58, 15, 59, 62, 10, 67,  3, 79,  9, 69, 95, 35, 49, 72, 76, 87, 41,\n",
            "         61, 73, 79, 84, 45, 67, 87, 45, 49, 50, 52, 68, 25, 96]])\n"
          ]
        }
      ],
      "source": [
        "# larger context in the form of reversed sequence\n",
        "batch_size = 16\n",
        "seq_len = 50\n",
        "vocab_size = 100\n",
        "x = torch.randint(0, vocab_size, (batch_size,seq_len))\n",
        "# reverse sequence as target\n",
        "y = torch.flip(x, dims=(-1,))\n",
        "\n",
        "print(\"Input sequences:\\n\", x)\n",
        "print(\"Target (reversed) sequences:\\n\", y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 4.744674205780029\n",
            "1 4.48797082901001\n",
            "2 4.23170280456543\n",
            "3 3.927212715148926\n",
            "4 3.7159228324890137\n",
            "5 3.534147024154663\n",
            "6 3.3498973846435547\n",
            "7 3.208298683166504\n",
            "8 2.814455270767212\n",
            "9 2.6157360076904297\n",
            "10 2.432138681411743\n",
            "11 2.062748670578003\n",
            "12 1.8448920249938965\n",
            "13 1.5506446361541748\n",
            "14 1.241349458694458\n",
            "15 1.0094467401504517\n",
            "16 0.765720546245575\n",
            "17 0.5727940201759338\n",
            "18 0.4164573550224304\n",
            "19 0.2993224263191223\n",
            "20 0.2102164328098297\n",
            "21 0.14678436517715454\n",
            "22 0.10846157371997833\n",
            "23 0.08212029188871384\n",
            "24 0.06059759482741356\n",
            "25 0.045069798827171326\n",
            "26 0.035211049020290375\n",
            "27 0.02803529053926468\n",
            "28 0.022875657305121422\n",
            "29 0.019254906103014946\n",
            "30 0.0159899964928627\n",
            "31 0.013663635589182377\n",
            "32 0.011799123138189316\n",
            "33 0.010077040642499924\n",
            "34 0.008975471369922161\n",
            "35 0.007924433797597885\n",
            "36 0.007059736642986536\n",
            "37 0.006503175478428602\n",
            "38 0.005859775003045797\n",
            "39 0.005433380138128996\n",
            "40 0.0050832186825573444\n",
            "41 0.0046897088177502155\n",
            "42 0.004481224808841944\n",
            "43 0.004209522157907486\n",
            "44 0.003995881881564856\n",
            "45 0.0038514670450240374\n",
            "46 0.003644816344603896\n",
            "47 0.003543482394888997\n",
            "48 0.0034040382597595453\n",
            "49 0.0032910467125475407\n",
            "50 0.0032189879566431046\n",
            "51 0.0031071847770363092\n",
            "52 0.003058810019865632\n",
            "53 0.0029781546909362078\n",
            "54 0.0029217859264463186\n",
            "55 0.002876708749681711\n",
            "56 0.002813994185999036\n",
            "57 0.002786122029647231\n",
            "58 0.0027324617840349674\n",
            "59 0.0027035230305045843\n",
            "60 0.0026676026172935963\n",
            "61 0.0026328801177442074\n",
            "62 0.0026110850740224123\n",
            "63 0.002576017053797841\n",
            "64 0.002559653017669916\n",
            "65 0.002530827419832349\n",
            "66 0.0025139139033854008\n",
            "67 0.002493534004315734\n",
            "68 0.002474764594808221\n",
            "69 0.0024609696120023727\n",
            "70 0.002442082157358527\n",
            "71 0.002431736560538411\n",
            "72 0.002414732240140438\n",
            "73 0.002405415056273341\n",
            "74 0.0023913162294775248\n",
            "75 0.0023818286135792732\n",
            "76 0.0023706769570708275\n",
            "77 0.0023608473129570484\n",
            "78 0.002352010225877166\n",
            "79 0.002342220162972808\n",
            "80 0.0023349199909716845\n",
            "81 0.002325588371604681\n",
            "82 0.002319189254194498\n",
            "83 0.002310606185346842\n",
            "84 0.0023047011345624924\n",
            "85 0.0022969702258706093\n",
            "86 0.002291354350745678\n",
            "87 0.0022844490595161915\n",
            "88 0.002279052045196295\n",
            "89 0.0022728689946234226\n",
            "90 0.002267664996907115\n",
            "91 0.00226207566447556\n",
            "92 0.002257081912830472\n",
            "93 0.002251966390758753\n",
            "94 0.0022472005803138018\n",
            "95 0.0022424571216106415\n",
            "96 0.0022379187867045403\n",
            "97 0.00223348056897521\n",
            "98 0.0022291564382612705\n",
            "99 0.0022249650210142136\n"
          ]
        }
      ],
      "source": [
        "steps = 100\n",
        "\n",
        "model = TransformerDecoder(vocabulary_size=vocab_size,max_seq_len=max_seq_len,\n",
        "                            d_model=64,num_heads=8,d_ff=512,num_blocks=3,\n",
        "                            )\n",
        "mask = make_mask(x.shape[1])\n",
        "\n",
        "\n",
        "opt = torch.optim.Adam(model.parameters(),lr=.01)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "for step in range(steps):\n",
        "    model.train()\n",
        "    opt.zero_grad()\n",
        "\n",
        "    y_pred = model(x,mask)\n",
        "    loss = loss_fn(y_pred.view(-1,y_pred.shape[-1]),y.view(-1))\n",
        "    loss.backward()\n",
        "\n",
        "    opt.step()\n",
        "\n",
        "    print(step,loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[45, 48, 53, 52, 75, 12, 37,  1, 82, 42, 64, 14,  5, 89, 59, 67, 22, 93,\n",
              "         38, 97, 97, 29, 70, 21,  2, 84,  9, 39, 83, 22,  5, 69, 50, 45, 95, 11,\n",
              "          9, 97, 52, 83, 73, 45, 43, 90, 25, 30, 46, 64, 73, 82],\n",
              "        [69, 57, 52, 94, 61, 83, 13, 76, 16, 99, 94, 64,  6, 14, 63, 80, 80, 18,\n",
              "         38, 50, 17, 22, 93, 42, 96, 22, 47, 75, 60, 15, 54, 69, 36, 48, 39, 65,\n",
              "         28, 57, 57, 68, 25,  6,  1, 63, 52,  4, 55, 78, 64, 59],\n",
              "        [ 2, 97, 40, 52, 19, 62, 59, 93,  1, 36, 67, 29, 99, 26, 14, 91, 19, 21,\n",
              "         21, 16, 57, 84, 45, 71,  3, 59, 22,  8, 53, 49, 89, 53, 14,  6, 68, 98,\n",
              "          9, 63, 58, 93,  8, 19, 35,  3, 70, 44,  1, 84, 26, 82],\n",
              "        [92, 98, 63, 91, 26, 57, 95, 76,  9, 23, 50, 88, 65,  3, 21, 59, 97, 69,\n",
              "         16,  3, 28, 35, 41, 41, 53, 51, 44, 89, 59, 78, 68, 71, 62, 43, 37, 72,\n",
              "         24,  6, 92, 92, 41, 26,  1,  2, 68, 97, 94,  0, 61, 33],\n",
              "        [59, 11, 31, 31, 93, 95, 44,  1, 22, 42, 83,  9, 83,  9, 90, 93, 66,  7,\n",
              "         31, 68, 19, 81, 59, 28, 66, 83, 92, 21, 95, 39, 99, 62, 72, 70, 78, 76,\n",
              "         14, 66, 75, 66, 98, 11,  2, 12, 93,  9, 89, 93, 44, 97],\n",
              "        [93,  4, 56, 91,  7, 33, 87,  7, 50, 60, 93, 99, 46,  4, 67, 92, 19, 35,\n",
              "         84, 10, 75,  1, 82, 12, 47, 28, 60, 70, 71, 16, 50, 24,  4, 78, 87, 96,\n",
              "         61, 93, 31, 11, 10,  7, 16, 74, 73, 50, 80, 21, 78, 46],\n",
              "        [ 6, 44, 69, 36, 60, 38, 42, 15, 52, 38, 29, 88,  6, 50, 31, 63, 79, 51,\n",
              "         78, 53, 30, 63, 39, 96, 87, 47, 60, 11, 89, 91, 64, 87, 26, 28, 77,  7,\n",
              "         22, 44, 92, 48,  7, 53,  6, 79, 36, 29, 38, 97,  6, 19],\n",
              "        [69, 96, 31,  1, 22, 28,  9, 50,  1, 65, 51, 22, 44,  0, 67, 86, 95, 21,\n",
              "         46, 40, 18, 27, 28, 19, 59, 97, 54, 83, 64, 51,  2, 57, 37, 96, 72, 78,\n",
              "         49, 70, 19, 77, 15, 95, 92, 29,  0,  8, 63, 94, 84, 61],\n",
              "        [76, 26,  7, 20, 29, 33,  9, 99, 19, 22, 97, 37, 46, 40, 21, 93, 73, 17,\n",
              "         27, 16, 20,  7, 72, 83,  2, 10, 66, 27, 86, 90, 52, 36, 39, 48,  7,  4,\n",
              "         62,  1, 72, 17,  2, 30, 54, 29, 73, 98, 84, 97, 20,  5],\n",
              "        [92, 45, 69, 69, 59,  8, 72, 76, 77,  6, 73, 87, 66, 31, 24, 97, 97,  2,\n",
              "         91,  3, 41, 81, 71, 75, 70, 62, 96, 42, 81, 55, 58, 69, 82, 26, 61, 88,\n",
              "         22, 37,  4, 85, 72, 72, 49, 34, 36, 42, 97, 91, 67, 80],\n",
              "        [47, 23, 41,  3, 75, 29, 79, 76, 90, 74, 93,  9, 28, 71, 96, 80, 75, 87,\n",
              "         30, 26, 27, 48, 39, 31, 12, 56, 55, 11, 11,  9, 45, 76, 64, 75, 39,  5,\n",
              "         31, 92, 52, 75, 61, 27, 49, 80, 19, 30, 32, 41, 60, 27],\n",
              "        [30, 51, 36, 30, 59, 23, 31, 78, 92, 97, 23, 69, 28, 96, 31,  6, 93,  7,\n",
              "         95, 41,  7, 31, 62, 98, 86, 87, 61,  0, 43, 49,  6, 90, 83, 95, 26, 56,\n",
              "         84,  2, 67, 97, 51, 36, 30, 95, 85,  9, 96, 92, 52, 36],\n",
              "        [ 7, 58, 29, 83, 46,  3, 95, 35,  3,  7, 15, 39, 26, 89, 40, 63, 29, 55,\n",
              "         26,  3, 56, 96, 66, 26,  2, 90, 50,  3, 17, 70,  5, 67, 42, 36, 12, 70,\n",
              "         21, 60, 89, 54, 54, 79, 25, 13, 60, 61, 29, 36, 21, 14],\n",
              "        [22, 23, 76, 73, 66, 12, 92, 15,  0, 61, 19, 83, 87, 26, 55, 88, 28, 25,\n",
              "         13, 96, 20, 91, 52, 64, 87, 42, 68, 36, 69, 18, 29,  6, 72, 45, 68, 53,\n",
              "         20, 98, 38, 81, 57, 69, 22, 88, 76, 83, 22, 79, 29, 71],\n",
              "        [82, 96, 24, 71,  7, 17, 72, 68, 89, 31, 60, 27,  5, 72,  4, 74,  7, 48,\n",
              "         41, 16, 77,  5, 37,  9,  6, 33, 31, 89, 97, 93, 57, 50, 23, 99,  9, 70,\n",
              "         40, 37, 46, 95, 53, 92, 76, 22, 31, 58, 54, 58, 98,  2],\n",
              "        [22,  3, 16, 60, 22, 12, 52, 56, 89, 26, 44, 69, 27, 16, 67, 57, 73, 35,\n",
              "         21, 58, 15, 59, 62, 10, 67, 45, 79,  9, 69, 95, 35, 49, 72, 76, 87, 41,\n",
              "         61, 73, 79, 84, 45, 67, 87, 45, 60, 50, 52, 68, 25, 96]])"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.argmax(model(x),-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Increasing the length of the sequence led to the model being unable to learn the pattern.  This is likely due to the model's limited capacity to attend over long sequences."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
